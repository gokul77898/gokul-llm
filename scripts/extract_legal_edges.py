#!/usr/bin/env python3
"""
Extract Legal Edges CLI

Phase 2: Legal Edge Extraction

Extracts legal relationships from document text using rule-based patterns.
NO LLMs, NO embeddings, NO ML inference - pure pattern matching.

Usage:
    python scripts/extract_legal_edges.py
    python scripts/extract_legal_edges.py --graph-path data/graph/legal_graph_v1.pkl
    python scripts/extract_legal_edges.py --chunks-dir data/rag/chunks --version v2

Output:
    - data/graph/legal_graph_v2.pkl (pickle)
    - data/graph/legal_graph_v2.json (JSON for inspection)
"""

import argparse
import logging
import sys
from pathlib import Path

# Add project root to path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from src.graph.legal_edge_extractor import (
    LegalEdgeExtractor,
    RelationType,
    ExtractionStats,
)
from src.graph.legal_graph_builder import LegalGraphBuilder


def setup_logging(verbose: bool = False) -> None:
    """Configure logging."""
    level = logging.DEBUG if verbose else logging.INFO
    logging.basicConfig(
        level=level,
        format="%(asctime)s | %(levelname)-8s | %(message)s",
        datefmt="%Y-%m-%d %H:%M:%S",
    )


def print_banner() -> None:
    """Print CLI banner."""
    print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘           LEGAL EDGE EXTRACTOR - Phase 2                      â•‘
â•‘           Rule-Based Relationship Extraction                  â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)


def print_stats(stats: ExtractionStats) -> None:
    """Print extraction statistics."""
    print("\n" + "â”€" * 60)
    print("EXTRACTION STATISTICS")
    print("â”€" * 60)
    
    print(f"\nğŸ“Š Processing:")
    print(f"   Chunks processed: {stats.total_chunks_processed}")
    print(f"   Sentences processed: {stats.total_sentences_processed}")
    print(f"   Chunks with edges: {stats.chunks_with_edges}")
    
    print(f"\nğŸ”— Edges Extracted: {stats.total_edges_extracted}")
    print(f"   Duplicates skipped: {stats.duplicate_edges_skipped}")
    
    if stats.edges_by_type:
        print("\nğŸ“ˆ Edges by Type:")
        
        # Case â†’ Section relations
        section_types = [
            RelationType.INTERPRETS_SECTION.value,
            RelationType.APPLIES_SECTION.value,
            RelationType.DISTINGUISHES_SECTION.value,
        ]
        section_total = sum(stats.edges_by_type.get(t, 0) for t in section_types)
        
        if section_total > 0:
            print("\n   Case â†’ Section:")
            for edge_type in section_types:
                count = stats.edges_by_type.get(edge_type, 0)
                if count > 0:
                    print(f"     â†’ {edge_type}: {count}")
        
        # Case â†’ Case relations
        case_types = [
            RelationType.CITES_CASE.value,
            RelationType.OVERRULES_CASE.value,
        ]
        case_total = sum(stats.edges_by_type.get(t, 0) for t in case_types)
        
        if case_total > 0:
            print("\n   Case â†’ Case:")
            for edge_type in case_types:
                count = stats.edges_by_type.get(edge_type, 0)
                if count > 0:
                    print(f"     â†’ {edge_type}: {count}")
    
    print(f"\nâ±ï¸  Extraction Time: {stats.extraction_time_seconds}s")
    print(f"ğŸ“… Timestamp: {stats.extraction_timestamp}")


def print_graph_stats(builder: LegalGraphBuilder) -> None:
    """Print updated graph statistics."""
    stats = builder.get_stats()
    
    print("\n" + "â”€" * 60)
    print("UPDATED GRAPH STATISTICS")
    print("â”€" * 60)
    
    print(f"\nğŸ“Š Total Nodes: {stats.total_nodes}")
    print(f"ğŸ“Š Total Edges: {stats.total_edges}")
    
    print("\nğŸ“¦ Nodes by Type:")
    for node_type, count in stats.nodes_by_type.items():
        icon = {"act": "ğŸ“œ", "section": "ğŸ“„", "case": "âš–ï¸"}.get(node_type, "â€¢")
        print(f"   {icon} {node_type.upper()}: {count}")
    
    print("\nğŸ”— Edges by Type:")
    for edge_type, count in sorted(stats.edges_by_type.items()):
        print(f"   â†’ {edge_type}: {count}")


def print_validation_summary(extractor: LegalEdgeExtractor) -> None:
    """Print validation summary."""
    print("\n" + "â”€" * 60)
    print("VALIDATION SUMMARY")
    print("â”€" * 60)
    
    result = extractor.builder.validate()
    
    if result.is_valid:
        print("\nâœ… Graph is VALID")
    else:
        print("\nâš ï¸  Graph has issues")
    
    if result.orphan_sections:
        print(f"\n   Orphan sections: {len(result.orphan_sections)}")
    
    if result.cases_without_act:
        print(f"   Cases without act: {len(result.cases_without_act)}")
    
    if result.cases_without_section:
        print(f"   Cases without section: {len(result.cases_without_section)}")
    
    # Source coverage
    edges = extractor.get_extracted_edges()
    if edges:
        unique_chunks = set(e.source_chunk_id for e in edges)
        print(f"\nğŸ“Š Source Coverage:")
        print(f"   Unique source chunks: {len(unique_chunks)}")
        print(f"   Total edges with provenance: {len(edges)}")


def verify_deterministic_rebuild(
    extractor: LegalEdgeExtractor,
    documents_dir: str,
    chunks_dir: str,
    graph_path: str,
    output_dir: str,
) -> bool:
    """
    Verify that re-extraction produces the same edges.
    
    Returns:
        True if deterministic
    """
    print("\n" + "â”€" * 60)
    print("DETERMINISTIC REBUILD CHECK")
    print("â”€" * 60)
    
    # Get current counts
    original_edges = extractor._stats.total_edges_extracted
    original_graph_edges = extractor.builder.graph.number_of_edges()
    
    # Create new extractor and re-extract
    rebuild_extractor = LegalEdgeExtractor(
        documents_dir=documents_dir,
        chunks_dir=chunks_dir,
        graph_path=graph_path,
        output_dir=output_dir,
    )
    rebuild_extractor.extract()
    
    rebuild_edges = rebuild_extractor._stats.total_edges_extracted
    
    is_deterministic = original_edges == rebuild_edges
    
    if is_deterministic:
        print(f"\nâœ… Extraction is DETERMINISTIC")
        print(f"   Original: {original_edges} edges extracted")
        print(f"   Rebuild:  {rebuild_edges} edges extracted")
    else:
        print(f"\nâŒ Extraction is NOT deterministic!")
        print(f"   Original: {original_edges} edges extracted")
        print(f"   Rebuild:  {rebuild_edges} edges extracted")
    
    return is_deterministic


def main():
    """Main entry point."""
    parser = argparse.ArgumentParser(
        description="Extract legal relationships from document text",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
    # Extract edges using default paths
    python scripts/extract_legal_edges.py
    
    # Extract with custom paths
    python scripts/extract_legal_edges.py \\
        --graph-path data/graph/legal_graph_v1.pkl \\
        --chunks-dir data/rag/chunks \\
        --version v2
    
    # Skip deterministic check (faster)
    python scripts/extract_legal_edges.py --skip-deterministic-check
    
    # Verbose output
    python scripts/extract_legal_edges.py --verbose
        """
    )
    
    parser.add_argument(
        "--documents-dir",
        type=str,
        default="data/rag/documents",
        help="Path to canonical documents directory"
    )
    parser.add_argument(
        "--chunks-dir",
        type=str,
        default="data/rag/chunks",
        help="Path to chunks directory"
    )
    parser.add_argument(
        "--graph-path",
        type=str,
        default="data/graph/legal_graph_v1.pkl",
        help="Path to existing graph (Phase 1)"
    )
    parser.add_argument(
        "--output-dir",
        type=str,
        default="data/graph",
        help="Path for graph output"
    )
    parser.add_argument(
        "--version",
        type=str,
        default="v2",
        help="Version string for output files"
    )
    parser.add_argument(
        "--skip-validation",
        action="store_true",
        help="Skip validation step"
    )
    parser.add_argument(
        "--skip-deterministic-check",
        action="store_true",
        help="Skip deterministic rebuild check"
    )
    parser.add_argument(
        "--verbose", "-v",
        action="store_true",
        help="Enable verbose logging"
    )
    
    args = parser.parse_args()
    
    # Setup
    setup_logging(args.verbose)
    print_banner()
    
    logger = logging.getLogger(__name__)
    
    # Print configuration
    print(f"ğŸ“‚ Documents directory: {args.documents_dir}")
    print(f"ğŸ“‚ Chunks directory: {args.chunks_dir}")
    print(f"ğŸ“‚ Input graph: {args.graph_path}")
    print(f"ğŸ“‚ Output directory: {args.output_dir}")
    print(f"ğŸ“Œ Version: {args.version}")
    
    # Check paths
    graph_exists = Path(args.graph_path).exists()
    docs_exist = Path(args.documents_dir).exists()
    chunks_exist = Path(args.chunks_dir).exists()
    
    if graph_exists:
        print(f"\nâœ… Found existing graph at {args.graph_path}")
    else:
        print(f"\nâš ï¸  Graph not found at {args.graph_path}")
        print("   Will create new graph")
    
    if not docs_exist and not chunks_exist:
        print("\nâš ï¸  Warning: Neither documents nor chunks directory exists.")
        print("   No edges will be extracted.")
    else:
        if docs_exist:
            doc_count = len(list(Path(args.documents_dir).glob("*.json")))
            print(f"   Found {doc_count} document files")
        if chunks_exist:
            chunk_count = len([
                f for f in Path(args.chunks_dir).glob("*.json")
                if f.name != "index.json"
            ])
            print(f"   Found {chunk_count} chunk files")
    
    # Create extractor
    print("\nğŸ”¨ Extracting legal relationships...")
    
    extractor = LegalEdgeExtractor(
        documents_dir=args.documents_dir,
        chunks_dir=args.chunks_dir,
        graph_path=args.graph_path,
        output_dir=args.output_dir,
    )
    
    # Run extraction
    stats = extractor.extract()
    print_stats(stats)
    
    # Print graph stats
    print_graph_stats(extractor.builder)
    
    # Validation
    if not args.skip_validation:
        print_validation_summary(extractor)
    
    # Deterministic check
    if not args.skip_deterministic_check and stats.total_edges_extracted > 0:
        verify_deterministic_rebuild(
            extractor,
            args.documents_dir,
            args.chunks_dir,
            args.graph_path,
            args.output_dir,
        )
    
    # Save graph
    print("\n" + "â”€" * 60)
    print("SAVING GRAPH")
    print("â”€" * 60)
    
    pickle_path, json_path = extractor.save(version=args.version)
    
    print(f"\nâœ… Graph saved successfully!")
    print(f"   ğŸ“¦ Pickle: {pickle_path}")
    print(f"   ğŸ“„ JSON:   {json_path}")
    
    # Final summary
    print("\n" + "â•" * 60)
    print("EXTRACTION COMPLETE")
    print("â•" * 60)
    
    graph_stats = extractor.builder.get_stats()
    print(f"\nğŸ“Š Summary:")
    print(f"   â€¢ Edges extracted: {stats.total_edges_extracted}")
    print(f"   â€¢ Graph nodes: {graph_stats.total_nodes}")
    print(f"   â€¢ Graph edges: {graph_stats.total_edges}")
    print(f"   â€¢ Time: {stats.extraction_time_seconds}s")
    
    if stats.total_edges_extracted == 0:
        print("\nâš ï¸  No edges extracted. To populate:")
        print("   1. Add case_law documents to data/rag/documents/")
        print("   2. Add case_law chunks to data/rag/chunks/")
        print("   3. Re-run this script")
    
    print("\nâœ¨ Ready for Phase 3 (Graph Traversal)!")
    print()


if __name__ == "__main__":
    main()
