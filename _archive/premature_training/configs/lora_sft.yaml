# LoRA Fine-Tuning Configuration
# For supervised fine-tuning (SFT) on legal QA data

# Model configuration
model:
  name: "mamba"  # or "transformer"
  base_checkpoint: null  # Use default from registry
  
# LoRA configuration
lora:
  r: 8  # LoRA rank
  lora_alpha: 16  # LoRA alpha
  lora_dropout: 0.05
  target_modules:  # Modules to apply LoRA
    - "c_attn"
    - "c_proj"
  bias: "none"
  task_type: "CAUSAL_LM"

# Training configuration
training:
  # Safety: dry_run=true prevents actual training
  dry_run: true
  epochs: 0  # Set to >0 only when ready to train
  batch_size: 4
  gradient_accumulation_steps: 4
  learning_rate: 2.0e-4
  warmup_steps: 100
  max_steps: -1  # -1 means use epochs
  logging_steps: 10
  save_steps: 500
  eval_steps: 250
  max_grad_norm: 1.0
  weight_decay: 0.01
  
# Dataset configuration
data:
  train_file: "data/train_sft.jsonl"
  val_file: "data/val_sft.jsonl"
  max_seq_length: 512
  
# Output configuration
output:
  checkpoint_dir: "checkpoints/lora/"
  model_name: "mamba_lora"
  save_total_limit: 3
  
# Hardware configuration
hardware:
  device: "auto"  # auto, cpu, cuda, mps
  fp16: false  # Enable for GPU
  bf16: false  # Enable for newer GPUs
  
# Logging
logging:
  report_to: "none"  # tensorboard, wandb, none
  logging_dir: "logs/lora_training"
