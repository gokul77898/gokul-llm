# RAG System Configuration with LangChain

retriever:
  embedding_model: "sentence-transformers/all-MiniLM-L6-v2"
  # Alternative: "sentence-transformers/all-mpnet-base-v2"
  top_k: 5
  use_reranking: true
  reranker_model: null  # "cross-encoder/ms-marco-MiniLM-L-6-v2"
  
document_store:
  type: "faiss"  # "faiss", "chroma", or "hybrid"
  index_type: "Flat"  # "Flat", "IVFFlat", "HNSW"
  dimension: null  # Auto-detect from embedding model
  persist_directory: "./data/document_store"

generator:
  model_name: "Qwen/Qwen2.5-32B-Instruct"  # Authorized decoder model
  # Phase 3.6: Unified to Qwen 2.5-32B-Instruct
  max_context_length: 2048
  max_generation_length: 512
  temperature: 0.7
  top_p: 0.9
  top_k: 50
  use_citations: true
  use_chain_of_thought: false

pipeline:
  cache_results: true
  num_docs_to_use: 3  # How many retrieved docs to use for generation
  
training:
  checkpoint_dir: "./checkpoints/rag"
  num_epochs: 3
  batch_size: 8
  learning_rate: 1e-5

evaluation:
  metrics: ["rouge", "bleu", "relevance"]
  eval_queries: []

langchain:
  enabled: true
  memory_type: "conversation_buffer"  # "conversation_buffer", "conversation_summary"
  max_memory_length: 10
