# AUTO-DETECTING Mamba Configuration
# Automatically selects the best backend based on platform:
#   - Mac (darwin) → Mamba2
#   - Windows/Linux + CUDA → REAL Mamba SSM
#   - No GPU → Fallback to Transformer

mamba:
  # Prefer REAL Mamba SSM when available (CUDA required)
  prefer_real: true
  
  # REAL Mamba SSM model (for Windows/Linux + CUDA)
  real_model: "state-spaces/mamba-130m"
  # Options: mamba-130m, mamba-370m, mamba-790m, mamba-1.4b, mamba-2.8b
  
  # Mamba2 model (for Mac)
  mamba2_model: "mamba2-base"
  # Options: mamba2-base, mamba2-medium, mamba2-large
  
  # Fallback model when no Mamba backend available
  fallback: "transformer"
  
  # Master switch to enable/disable Mamba (can also use ENABLE_MAMBA env var)
  enable_mamba: true

model:
  # Architecture is auto-detected
  architecture: "mamba_auto"
  
  # Base models for each backend
  base_model: "state-spaces/mamba-130m"  # Used by REAL Mamba SSM
  mamba2_model: "mamba2-base"             # Used by Mamba2
  
  # Long context support (both backends handle long sequences efficiently)
  max_length: 2048  # Can be increased up to 16k+
  
  # Model parameters (set by pretrained models)
  d_model: 768
  n_layers: 24
  vocab_size: 50277

system:
  device: "auto"  # auto-detect CUDA/MPS/CPU
  dtype: "float32"

training:
  # LoRA fine-tuning (target modules auto-selected based on backend)
  lora_enabled: true
  lora_r: 8
  lora_alpha: 32
  lora_dropout: 0.1
  
  # Target modules are auto-selected:
  # - REAL Mamba SSM: ["in_proj", "out_proj", "x_proj", "dt_proj"]
  # - Mamba2: ["mixer.Wq", "mixer.Wk", "mixer.Wv"]
  # - Transformer: ["c_attn", "c_proj"]
  
  batch_size: 4
  learning_rate: 1e-4
  gradient_accumulation_steps: 4
  max_grad_norm: 1.0
  warmup_steps: 100

generation:
  max_new_tokens: 512
  temperature: 0.7
  top_p: 0.9
  top_k: 50
  do_sample: true

# Backend selection logic
backend_selection:
  strategy: "auto"  # Options: auto, force-real-mamba, force-mamba2, disable
  
  # Platform-specific preferences
  darwin:  # Mac
    preferred_backend: "mamba2"
    reason: "Mamba2 optimized for Mac MPS"
  
  linux_cuda:
    preferred_backend: "real-mamba"
    reason: "REAL Mamba SSM optimized for CUDA"
  
  windows_cuda:
    preferred_backend: "real-mamba"
    reason: "REAL Mamba SSM optimized for CUDA"
  
  no_gpu:
    preferred_backend: "none"
    fallback_to: "transformer"
    reason: "No GPU available, use Transformer"

# Performance notes
performance:
  real_mamba_ssm:
    platform: "Linux/Windows + CUDA"
    complexity: "O(n)"
    speed: "Fast"
    memory: "Lower"
    install: "pip install mamba-ssm causal-conv1d>=1.2.0"
  
  mamba2:
    platform: "Mac (MPS/CPU)"
    complexity: "O(n)"
    speed: "Good"
    memory: "Lower"
    install: "pip install mamba2"
  
  transformer:
    platform: "All (fallback)"
    complexity: "O(n²)"
    speed: "Slower for long contexts"
    memory: "Higher"
    install: "Already installed"

# Troubleshooting
troubleshooting:
  mamba_unavailable: |
    If Mamba is not loading:
    1. Check platform: Mac → install mamba2, Linux/Windows+CUDA → install mamba-ssm
    2. Verify CUDA available: python -c "import torch; print(torch.cuda.is_available())"
    3. Check backend detection: python -c "from src.core.mamba_loader import get_mamba_info; print(get_mamba_info())"
    4. Set ENABLE_MAMBA=true environment variable
  
  slow_performance: |
    If generation is slow:
    1. Ensure using correct backend (CUDA for best performance)
    2. On Mac without GPU: System will use CPU fallback
    3. Consider disabling Mamba and using Transformer if no GPU
