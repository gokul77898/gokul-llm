model:
  base_model: "mamba"
  d_model: 128
  max_length: 256
  dropout: 0.1

training:
  batch_size: 4
  num_epochs: 1
  learning_rate: 0.0001
  weight_decay: 0.01
  max_grad_norm: 1.0
  gradient_accumulation_steps: 1
  logging_steps: 10
  save_steps: 100
  mixed_precision: false
  num_workers: 0

data:
  train_file: "data/rl_train.jsonl"
  val_file: "data/rl_val.jsonl"
  max_samples: 50

paths:
  checkpoint_dir: "checkpoints/rlhf/sft"
  log_dir: "logs/rlhf/sft"

system:
  device: "cpu"
  seed: 42
  use_wandb: false
