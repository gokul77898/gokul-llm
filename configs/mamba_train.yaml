# Mamba Model Training Configuration
model:
  vocab_size: 5000
  d_model: 256
  n_heads: 8
  n_layers: 4
  d_ff: 1024
  max_length: 512
  chunk_size: 128
  chunk_overlap: 32
  dropout: 0.1

training:
  batch_size: 8
  num_epochs: 10
  learning_rate: 0.0001
  weight_decay: 0.01
  warmup_steps: 500
  gradient_accumulation_steps: 2
  max_grad_norm: 1.0
  eval_steps: 100
  save_steps: 500
  logging_steps: 50
  mixed_precision: true
  num_workers: 4

data:
  train_file: "data/train.txt"
  val_file: "data/val.txt"
  max_samples: 1000
  min_doc_length: 100

paths:
  checkpoint_dir: "checkpoints/mamba"
  log_dir: "logs/mamba"
  vocab_file: "checkpoints/mamba/vocab.json"

system:
  device: "cuda"
  seed: 42
  use_wandb: false
  wandb_project: "mark-mamba"
