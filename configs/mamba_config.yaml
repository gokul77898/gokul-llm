# Mamba Model Configuration for Long Document Processing

model:
  vocab_size: 30000
  d_model: 512
  num_layers: 6
  num_heads: 8
  d_ff: 2048  # If null, defaults to d_model * 4
  max_length: 512
  chunk_size: 256
  chunk_overlap: 64
  dropout: 0.1
  num_classes: 5  # For classification task, null for generation
  use_hierarchical: true
  positional_encoding: "absolute"  # "absolute" or "learned"

tokenizer:
  min_freq: 2
  lowercase: true

training:
  task: "classification"  # "classification" or "generation"
  num_epochs: 10
  batch_size: 16
  learning_rate: 5e-5
  max_grad_norm: 1.0
  accumulation_steps: 1
  num_workers: 4
  checkpoint_dir: "./checkpoints/mamba"
  log_interval: 100
  eval_interval: 1000
  save_interval: 5000

optimizer:
  type: "adamw"
  betas: [0.9, 0.999]
  eps: 1e-8
  weight_decay: 0.01

scheduler:
  type: "cosine"
  warmup_ratio: 0.1
  min_lr: 1e-7

evaluation:
  metrics: ["accuracy", "f1", "precision", "recall"]
