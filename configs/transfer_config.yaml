# Transfer Learning Configuration for Legal Data

model:
  base_model: "law-ai/Indian-Legal-Assistant-8B"  # Phase 3.6: Authorized encoder
  task: "classification"  # "classification", "ner", "summarization", "generation", "qa"
  num_labels: 3
  max_length: 512
  dropout: 0.1
  freeze_base: false  # Whether to freeze base model weights

tokenizer:
  add_legal_tokens: true
  preprocess: true  # Preprocess legal entities

training:
  num_epochs: 5
  batch_size: 16
  learning_rate: 2e-5
  weight_decay: 0.01
  warmup_steps: 500  # If 0, uses 10% of total steps
  max_grad_norm: 1.0
  num_workers: 4
  checkpoint_dir: "./checkpoints/transfer"
  log_interval: 100
  eval_interval: 500
  save_interval: 2000

optimizer:
  type: "adamw"
  betas: [0.9, 0.999]
  eps: 1e-8

scheduler:
  type: "linear"
  warmup_ratio: 0.1

evaluation:
  metrics: ["accuracy", "f1", "precision", "recall"]
  
data:
  train_split: 0.8
  val_split: 0.1
  test_split: 0.1
  max_samples: null  # null for all data

augmentation:
  enabled: false
  techniques: []  # ["synonym_replacement", "back_translation"]
