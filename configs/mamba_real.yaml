# REAL Mamba SSM Configuration (State Space Model)
# Based on "Mamba: Linear-Time Sequence Modeling with Selective State Spaces"
# by Albert Gu and Tri Dao

model:
  # HuggingFace model name for REAL Mamba SSM
  base_model: "state-spaces/mamba-130m"  # Options: mamba-130m, mamba-370m, mamba-790m, mamba-1.4b, mamba-2.8b
  
  # Architecture is SSM (State Space Model), NOT Transformer
  architecture: "mamba_ssm"
  
  # Long context support (Mamba handles long sequences efficiently)
  max_length: 2048  # Can be increased up to 16k+ with Mamba
  
  # Model parameters (for reference, set by pretrained model)
  d_model: 768
  n_layers: 24
  vocab_size: 50277
  
  # SSM-specific settings
  ssm_cfg:
    d_state: 16
    d_conv: 4
    expand: 2
  
system:
  device: "auto"  # auto-detect CUDA/MPS/CPU
  dtype: "float32"  # Use float32 for stability

training:
  # For LoRA fine-tuning on REAL Mamba
  lora_enabled: true
  lora_r: 8
  lora_alpha: 32
  lora_dropout: 0.1
  # Mamba-specific LoRA target modules
  lora_target_modules:
    - "in_proj"      # Input projection
    - "out_proj"     # Output projection
    - "x_proj"       # State projection
    - "dt_proj"      # Delta projection
  
  batch_size: 4
  learning_rate: 1e-4
  gradient_accumulation_steps: 4
  max_grad_norm: 1.0
  warmup_steps: 100

generation:
  max_new_tokens: 512
  temperature: 0.7
  top_p: 0.9
  top_k: 50
  do_sample: true

# Performance notes
notes: |
  REAL Mamba SSM (State Space Model) features:
  - Linear O(n) complexity (vs O(nÂ²) for Transformers)
  - Efficient long-sequence processing
  - Selective state space mechanism
  - Hardware-optimized kernels (CUDA required for best performance)
  - Limited Mac MPS support (use CPU fallback)
  
  Install:
    pip install mamba-ssm causal-conv1d>=1.2.0
    pip install transformers accelerate einops
