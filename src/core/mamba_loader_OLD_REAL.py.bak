"""
REAL Mamba SSM Loader (State Space Model by Tri Dao)

Loads the authentic Mamba model from state-spaces/mamba, not custom transformers.
"""

import os
import logging
from typing import Optional, Dict, Any
from pathlib import Path

import torch

logger = logging.getLogger(__name__)


class MambaShim:
    """Shim object when real Mamba SSM is not available"""
    
    def __init__(self, reason: str):
        self.available = False
        self.reason = reason
        logger.warning(f"Real Mamba SSM not available: {reason}")
    
    def generate(self, *args, **kwargs):
        """Stub generation method"""
        raise RuntimeError(f"Real Mamba SSM not available: {self.reason}")


class RealMambaModel:
    """Wrapper for REAL Mamba SSM with generation interface"""
    
    def __init__(self, model, tokenizer, device):
        self.model = model
        self.tokenizer = tokenizer
        self.device = device
        self.available = True
        self.max_context_length = 2048  # Mamba can handle very long sequences
        
        logger.info(f"âœ… REAL Mamba SSM loaded on {device}")
    
    def generate_with_state_space(
        self, 
        prompt: str, 
        context: str = "",
        max_new_tokens: int = 256,
        temperature: float = 0.7,
        top_p: float = 0.9,
        top_k: int = 50,
        **kwargs
    ) -> str:
        """
        Generate answer using REAL Mamba's state space mechanism
        
        Args:
            prompt: Query prompt
            context: Retrieved context (Mamba handles long contexts efficiently)
            max_new_tokens: Maximum tokens to generate
            temperature: Sampling temperature
            top_p: Nucleus sampling parameter
            top_k: Top-k sampling
            
        Returns:
            Generated answer string
        """
        try:
            # Combine prompt and context
            full_text = f"{context}\n\nQuestion: {prompt}\nAnswer:"
            
            # Tokenize with Mamba's long context support
            inputs = self.tokenizer(
                full_text,
                return_tensors="pt",
                max_length=self.max_context_length,
                truncation=True,
                padding=True
            )
            
            # Move to device
            input_ids = inputs['input_ids'].to(self.device)
            
            # Generate using Mamba's efficient SSM forward pass
            with torch.no_grad():
                outputs = self.model.generate(
                    input_ids=input_ids,
                    max_length=input_ids.shape[1] + max_new_tokens,
                    temperature=temperature,
                    top_p=top_p,
                    top_k=top_k,
                    do_sample=True,
                    pad_token_id=self.tokenizer.eos_token_id,
                    eos_token_id=self.tokenizer.eos_token_id,
                    **kwargs
                )
            
            # Decode output
            generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            
            # Extract answer (remove prompt)
            if "Answer:" in generated_text:
                answer = generated_text.split("Answer:")[-1].strip()
            else:
                # Return everything after the input
                answer = generated_text[len(full_text):].strip()
            
            return answer
            
        except Exception as e:
            logger.error(f"Real Mamba generation failed: {e}")
            raise
    
    def generate(self, input_ids, **kwargs):
        """Direct generation interface for compatibility"""
        return self.model.generate(input_ids, **kwargs)


def load_real_mamba(
    model_name: str = "state-spaces/mamba-130m",
    config: Optional[Dict[str, Any]] = None
) -> RealMambaModel:
    """
    Load REAL Mamba SSM model
    
    Args:
        model_name: HuggingFace model name (default: state-spaces/mamba-130m)
        config: Optional config dict with device settings
        
    Returns:
        RealMambaModel instance or MambaShim if unavailable
    """
    config = config or {}
    
    # Check environment variable
    if os.getenv("ENABLE_MAMBA", "true").lower() == "false":
        logger.info("Real Mamba disabled via ENABLE_MAMBA env var")
        return MambaShim("Disabled via environment variable")
    
    # Check config setting
    if config and not config.get("enable_mamba", True):
        logger.info("Real Mamba disabled via config")
        return MambaShim("Disabled via configuration")
    
    try:
        # Import real Mamba SSM
        logger.info("ðŸ” Attempting to load REAL Mamba SSM...")
        
        from mamba_ssm.models.mixer_seq_simple import MambaLMHeadModel
        from transformers import AutoTokenizer
        
        logger.info("âœ… Real mamba-ssm package found")
        
        # Determine device
        if torch.cuda.is_available():
            device = torch.device("cuda")
            logger.info("ðŸš€ Using CUDA for Real Mamba SSM")
        elif torch.backends.mps.is_available():
            device = torch.device("cpu")  # MPS not fully supported by Mamba
            logger.warning("âš ï¸  MPS detected but not fully supported by Mamba SSM, using CPU")
            logger.warning("    For best performance, use CUDA-enabled GPU")
        else:
            device = torch.device("cpu")
            logger.warning("âš ï¸  Using CPU for Real Mamba (will be slow)")
        
        # Load tokenizer
        logger.info(f"ðŸ“¥ Loading tokenizer for {model_name}...")
        try:
            tokenizer = AutoTokenizer.from_pretrained(model_name)
        except:
            # Fallback to GPT2 tokenizer if model-specific not available
            logger.info("   Using GPT-2 tokenizer as fallback")
            tokenizer = AutoTokenizer.from_pretrained("gpt2")
        
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        logger.info("âœ… Tokenizer loaded")
        
        # Load REAL Mamba model
        logger.info(f"ðŸ“¥ Loading REAL Mamba SSM model: {model_name}...")
        
        try:
            # Try loading from HuggingFace
            model = MambaLMHeadModel.from_pretrained(
                model_name,
                device=str(device),
                dtype=torch.float32
            )
            logger.info(f"âœ… Loaded from HuggingFace: {model_name}")
        except Exception as e:
            logger.warning(f"   Failed to load from HF: {e}")
            logger.info("   Attempting to load pretrained Mamba-130m...")
            
            # Try loading default Mamba architecture
            from mamba_ssm.models.config_mamba import MambaConfig
            
            mamba_config = MambaConfig(
                d_model=768,
                n_layer=24,
                vocab_size=50277,
                ssm_cfg={},
                rms_norm=True,
                residual_in_fp32=True,
                fused_add_norm=True,
                pad_vocab_size_multiple=8,
            )
            
            model = MambaLMHeadModel(mamba_config, device=device)
            logger.info("âœ… Loaded with default Mamba architecture")
        
        model.eval()
        
        # Load checkpoint if specified
        checkpoint_path = config.get("checkpoint_path")
        if checkpoint_path and Path(checkpoint_path).exists():
            checkpoint = torch.load(checkpoint_path, map_location=device)
            if 'model_state_dict' in checkpoint:
                model.load_state_dict(checkpoint['model_state_dict'])
            else:
                model.load_state_dict(checkpoint)
            logger.info(f"âœ… Loaded checkpoint from {checkpoint_path}")
        
        logger.info("=" * 70)
        logger.info("  âœ… REAL MAMBA SSM LOADED SUCCESSFULLY")
        logger.info("=" * 70)
        logger.info(f"  Model: {model_name}")
        logger.info(f"  Device: {device}")
        logger.info(f"  Architecture: State Space Model (not Transformer)")
        logger.info(f"  Context: Efficient long-sequence processing")
        logger.info("=" * 70)
        
        return RealMambaModel(model, tokenizer, device)
        
    except ImportError as e:
        error_msg = str(e)
        logger.error("âŒ Failed to import real mamba-ssm package")
        logger.error(f"   Error: {error_msg}")
        logger.error("")
        logger.error("   To install REAL Mamba SSM:")
        logger.error("   pip install mamba-ssm causal-conv1d>=1.2.0")
        logger.error("   pip install transformers accelerate einops")
        logger.error("")
        logger.error("   Note: Requires CUDA on Linux/Windows")
        logger.error("   Mac MPS has limited support")
        logger.error("")
        return MambaShim(f"Import failed: {error_msg}")
    
    except Exception as e:
        logger.error(f"âŒ Failed to load Real Mamba SSM: {e}", exc_info=True)
        return MambaShim(f"Loading error: {str(e)}")


def load_mamba_model(config: Optional[Dict[str, Any]] = None) -> RealMambaModel:
    """
    Main entry point: Load REAL Mamba SSM model
    
    Args:
        config: Optional config with model_name, checkpoint_path, etc.
        
    Returns:
        RealMambaModel or MambaShim
    """
    config = config or {}
    model_name = config.get("model_name", "state-spaces/mamba-130m")
    
    return load_real_mamba(model_name, config)


def is_mamba_available() -> bool:
    """
    Quick check if REAL Mamba SSM is available
    
    Returns:
        bool: True if mamba-ssm package installed
    """
    try:
        import mamba_ssm
        return True
    except ImportError:
        return False


def get_mamba_info() -> Dict[str, Any]:
    """
    Get information about Mamba availability
    
    Returns:
        dict with availability status and details
    """
    info = {
        "available": is_mamba_available(),
        "package": "mamba-ssm (REAL State Space Model)",
        "cuda_available": torch.cuda.is_available(),
        "mps_available": torch.backends.mps.is_available(),
        "recommended_device": "cuda" if torch.cuda.is_available() else "cpu"
    }
    
    if info["available"]:
        try:
            import mamba_ssm
            info["version"] = getattr(mamba_ssm, "__version__", "unknown")
        except:
            pass
    
    return info
