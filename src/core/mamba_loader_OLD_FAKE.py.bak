"""
Mamba Model Loader with Safe Fallback

Provides loading and generation interface for Mamba (SSM) models.
If Mamba is unavailable, returns a shim object with available=False.
"""

import os
import logging
from typing import Optional, Dict, Any
from pathlib import Path

import torch

logger = logging.getLogger(__name__)


class MambaShim:
    """Shim object when Mamba is not available"""
    
    def __init__(self, reason: str):
        self.available = False
        self.reason = reason
        logger.warning(f"Mamba not available: {reason}")
    
    def generate_with_state_space(self, prompt: str, context: str, **kwargs) -> str:
        """Stub generation method"""
        raise RuntimeError(f"Mamba model not available: {self.reason}")


class MambaModel:
    """Wrapper for Mamba model with state space generation"""
    
    def __init__(self, model, tokenizer, device):
        self.model = model
        self.tokenizer = tokenizer
        self.device = device
        self.available = True
        self.max_context_length = 8192
    
    def generate_with_state_space(
        self, 
        prompt: str, 
        context: str = "",
        max_new_tokens: int = 256,
        temperature: float = 0.7,
        top_p: float = 0.9,
        **kwargs
    ) -> str:
        """
        Generate answer using Mamba's state space mechanism
        
        Args:
            prompt: Query prompt
            context: Retrieved context (can be very long)
            max_new_tokens: Maximum tokens to generate
            temperature: Sampling temperature
            top_p: Nucleus sampling parameter
            
        Returns:
            Generated answer string
        """
        try:
            # Combine prompt and context
            full_text = f"{context}\n\nQuestion: {prompt}\nAnswer:"
            
            # Tokenize with truncation for long context
            inputs = self.tokenizer(
                full_text,
                return_tensors="pt",
                max_length=self.max_context_length,
                truncation=True,
                padding=True
            ).to(self.device)
            
            # Generate using model
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=max_new_tokens,
                    temperature=temperature,
                    top_p=top_p,
                    do_sample=True,
                    pad_token_id=self.tokenizer.eos_token_id,
                    **kwargs
                )
            
            # Decode output
            generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            
            # Extract answer (remove prompt)
            if "Answer:" in generated_text:
                answer = generated_text.split("Answer:")[-1].strip()
            else:
                answer = generated_text.strip()
            
            return answer
            
        except Exception as e:
            logger.error(f"Mamba generation failed: {e}")
            raise


def load_mamba_model(config: Optional[Dict[str, Any]] = None) -> MambaModel:
    """
    Load Mamba model with safe fallback
    
    Args:
        config: Optional config dict with model settings
        
    Returns:
        MambaModel instance or MambaShim if unavailable
    """
    # Check environment variable
    if os.getenv("ENABLE_MAMBA", "true").lower() == "false":
        logger.info("Mamba disabled via ENABLE_MAMBA env var")
        return MambaShim("Disabled via environment variable")
    
    # Check config setting
    if config and not config.get("enable_mamba", True):
        logger.info("Mamba disabled via config")
        return MambaShim("Disabled via configuration")
    
    # Try to import Mamba
    try:
        # First try: custom Mamba implementation
        try:
            from src.mamba import MambaModel as CustomMamba, DocumentTokenizer
            logger.info("Using custom Mamba implementation")
            use_custom = True
        except ImportError:
            # Second try: mamba-ssm package
            try:
                import mamba_ssm
                from transformers import AutoTokenizer, AutoModelForCausalLM
                logger.info("Using mamba-ssm package")
                use_custom = False
            except ImportError:
                raise ImportError("No Mamba implementation found")
        
        # Determine device
        if torch.cuda.is_available():
            device = torch.device("cuda")
            logger.info("Using CUDA for Mamba")
        elif torch.backends.mps.is_available():
            device = torch.device("mps")
            logger.info("Using MPS (Mac GPU) for Mamba")
        else:
            device = torch.device("cpu")
            logger.warning("Using CPU for Mamba (may be slow)")
        
        # Load model based on implementation
        if use_custom:
            # Load custom Mamba model
            config = config or {}
            vocab_size = config.get("vocab_size", 50257)
            d_model = config.get("d_model", 512)
            n_layers = config.get("n_layers", 6)
            
            tokenizer = DocumentTokenizer(
                vocab_size=vocab_size,
                max_length=8192,
                chunk_size=128,
                chunk_overlap=32
            )
            
            model = CustomMamba(
                vocab_size=vocab_size,
                d_model=d_model,
                num_heads=8,
                num_layers=n_layers,
                d_ff=d_model * 4,
                max_seq_length=8192,
                dropout=0.1
            ).to(device)
            
            # Load checkpoint if specified
            checkpoint_path = config.get("checkpoint_path")
            if checkpoint_path and Path(checkpoint_path).exists():
                checkpoint = torch.load(checkpoint_path, map_location=device)
                if 'model_state_dict' in checkpoint:
                    model.load_state_dict(checkpoint['model_state_dict'])
                else:
                    model.load_state_dict(checkpoint)
                logger.info(f"Loaded Mamba checkpoint from {checkpoint_path}")
            else:
                logger.warning("No checkpoint found, using randomly initialized Mamba")
            
            model.eval()
            
        else:
            # Load HuggingFace-compatible Mamba
            model_name = config.get("model_name", "state-spaces/mamba-130m")
            tokenizer = AutoTokenizer.from_pretrained("gpt2")
            tokenizer.pad_token = tokenizer.eos_token
            
            model = AutoModelForCausalLM.from_pretrained(
                model_name,
                trust_remote_code=True
            ).to(device)
            
            model.eval()
        
        logger.info(f"âœ… Mamba model loaded successfully on {device}")
        return MambaModel(model, tokenizer, device)
        
    except ImportError as e:
        logger.warning(f"Mamba import failed: {e}")
        return MambaShim(f"Import failed: {str(e)}")
    
    except Exception as e:
        logger.error(f"Mamba loading failed: {e}", exc_info=True)
        return MambaShim(f"Loading error: {str(e)}")


def is_mamba_available() -> bool:
    """
    Quick check if Mamba is available
    
    Returns:
        bool: True if Mamba can be loaded
    """
    try:
        # Check for custom implementation
        from src.mamba import MambaModel
        return True
    except ImportError:
        pass
    
    try:
        # Check for mamba-ssm package
        import mamba_ssm
        return True
    except ImportError:
        pass
    
    return False
