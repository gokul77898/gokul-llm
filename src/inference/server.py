"""
Phase R5: MoE + RAG Integration Server

CRITICAL INVARIANTS:
1. Decoder NEVER sees raw user query alone
2. Decoder ONLY sees assembled context from R4
3. If RAG refuses → decoder is NOT called
4. MoE routing decides encoder + decoder
5. All refusals are server-side enforced
6. No hallucinated answers possible

PIPELINE ORDER:
Encoder → RAG Retrieval → RAG Validation → Context Assembly → Decoder

PRODUCTION REQUIREMENTS:
1. Legal disclaimer on ALL successful responses
2. Structured refusals with machine_reason + user_safe_message
3. No silent failures - all exceptions caught
4. All requests logged to production_requests.jsonl and rag_moe_pipeline.jsonl
5. Retry logic (exactly one retry) for HF API failures
"""

# Load environment variables FIRST
from dotenv import load_dotenv
load_dotenv()

import logging
import json
import os
import re
import time
import uuid
from datetime import datetime
from pathlib import Path
from fastapi import FastAPI, HTTPException, Request
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
from pydantic import BaseModel
from typing import List, Dict, Any, Optional

import requests

from src.inference.moe_router import MoERouter

# RAG imports (Phase R5)
from src.rag import (
    LegalRetriever,
    RetrievalValidator,
    ContextAssembler,
    BudgetConfig,
)

# Post-generation verifier (Phase R6b)
from src.inference.postgen_verifier import PostGenerationVerifier, VerificationStatus

# Trace & Replay (Phase R7) + Phase P5 GPU Memory
from src.inference.trace import (
    generate_trace_id,
    ReplayArtifact,
    ReplayStore,
    EncoderTrace,
    RetrievalTrace,
    ContextTrace,
    DecoderTrace,
    PostGenTrace,
    FinalResponse,
    DecisionProvenance,
    ProvenanceBuilder,
    DriftSignalLogger,
    log_with_trace,
    GPUMemoryTrace,
)

# Local Model Registry (GPU-Ready) + Phase P2 Exceptions + Phase P4 Health + Phase P5 Memory
from src.inference.local_models import (
    LocalModelRegistry,
    run_local_encoder,
    run_local_decoder,
    ensure_model_loaded,
    EncoderExecutionError,
    DecoderExecutionError,
    ModelState,
    get_gpu_health_info,
    get_gpu_memory_snapshot,
    GPUMemorySnapshot,
)

# Phase P6: Configuration Integrity & Kill Switches
# Phase P8: Canary Mode
# Phase P10: Ops Override
from src.inference.config_fingerprint import (
    get_config_hash,
    get_feature_flags,
    is_encoder_enabled,
    is_rag_enabled,
    is_decoder_enabled,
    validate_config_at_startup,
    is_canary_mode,
    get_canary_limits,
    is_ops_override_allowed,
)

# ─────────────────────────────────────────────
# PHASE P6: VALIDATE CONFIG AT STARTUP (FAIL-FAST)
# ─────────────────────────────────────────────
validate_config_at_startup()
print(f"[CONFIG] Hash: {get_config_hash()}")

# ─────────────────────────────────────────────
# Validate HF_TOKEN on startup (DEPRECATED - kept for backward compat)
# ─────────────────────────────────────────────
HF_TOKEN = os.getenv("HF_TOKEN")
if not HF_TOKEN:
    # No longer required - local models don't need HF_TOKEN
    print("[WARN] HF_TOKEN not configured (not required for local models)")
    HF_TOKEN = "not_required_for_local"
else:
    print("[OK] HF_TOKEN detected (deprecated - local models preferred)")

# ─────────────────────────────────────────────
# Local Model Registry (GPU-Ready, Lazy Load)
# ─────────────────────────────────────────────
MODEL_REGISTRY = LocalModelRegistry()
print("[INFO] Local model registry initialized (no models loaded)")

# ─────────────────────────────────────────────
# App setup
# ─────────────────────────────────────────────
app = FastAPI(title="MARK MoE Inference Server")

app.add_middleware(
    CORSMiddleware,
    allow_origins=[
        "http://localhost:3000",
        "http://127.0.0.1:3000",
        "http://localhost:3001",
        "http://127.0.0.1:3001",
    ],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

logger = logging.getLogger("InferenceServer")

# ─────────────────────────────────────────────
# PHASE 3: Production Constants
# ─────────────────────────────────────────────
LEGAL_DISCLAIMER = "This response is generated by an AI system for informational purposes only and does not constitute legal advice."

PRODUCTION_LOG_FILE = Path("logs/production_requests.jsonl")
PRODUCTION_LOG_FILE.parent.mkdir(parents=True, exist_ok=True)

MAX_RETRIES = 1  # Exactly one retry
RETRY_DELAY_MS = 500

# ─────────────────────────────────────────────
# PHASE P3: LATENCY BUDGETS & BACKPRESSURE
# ─────────────────────────────────────────────
# Global concurrency limit - fail fast, no queuing
MAX_CONCURRENT_REQUESTS: int = 2

# Total end-to-end request budget (milliseconds)
TOTAL_REQUEST_BUDGET_MS: int = 30_000

# Semaphore for concurrency control
import asyncio
_request_semaphore = asyncio.Semaphore(MAX_CONCURRENT_REQUESTS)


class LatencyTracker:
    """
    Track per-stage latency for a request.
    
    Phase P3: Enforces total request budget and tracks stage timings.
    """
    
    def __init__(self, budget_ms: int = TOTAL_REQUEST_BUDGET_MS):
        self.budget_ms = budget_ms
        self.start_time = time.time()
        self.timings: Dict[str, int] = {}
        self._stage_start: Optional[float] = None
        self._current_stage: Optional[str] = None
    
    def start_stage(self, stage: str) -> None:
        """Start timing a stage."""
        self._current_stage = stage
        self._stage_start = time.time()
    
    def end_stage(self) -> int:
        """End timing current stage, return duration in ms."""
        if self._stage_start is None or self._current_stage is None:
            return 0
        
        duration_ms = int((time.time() - self._stage_start) * 1000)
        self.timings[f"{self._current_stage}_ms"] = duration_ms
        self._stage_start = None
        self._current_stage = None
        return duration_ms
    
    def elapsed_ms(self) -> int:
        """Get total elapsed time in milliseconds."""
        return int((time.time() - self.start_time) * 1000)
    
    def remaining_ms(self) -> int:
        """Get remaining budget in milliseconds."""
        return max(0, self.budget_ms - self.elapsed_ms())
    
    def is_budget_exceeded(self) -> bool:
        """Check if total budget has been exceeded."""
        return self.elapsed_ms() > self.budget_ms
    
    def check_budget(self, stage: str) -> None:
        """
        Check budget before starting a stage.
        
        Raises:
            RuntimeError: If budget exceeded
        """
        if self.is_budget_exceeded():
            raise RuntimeError(
                f"Latency budget exceeded before {stage}: "
                f"{self.elapsed_ms()}ms > {self.budget_ms}ms"
            )
    
    def get_timings(self) -> Dict[str, int]:
        """Get all recorded timings plus total."""
        result = dict(self.timings)
        result["total_ms"] = self.elapsed_ms()
        return result

# ─────────────────────────────────────────────
# PHASE 3: Structured Refusal Helper
# ─────────────────────────────────────────────
REFUSAL_MESSAGES = {
    "missing_section": "Your query does not contain a specific legal section reference. Please include a section number (e.g., Section 420 IPC).",
    "encoder_failed": "We were unable to process your query at this time. Please try again later.",
    "decoder_failed": "We were unable to generate a response. Please try again later.",
    "rate_limited": "Service is temporarily busy. Please try again in a few moments.",
    "upstream_unavailable": "Our AI service is temporarily unavailable. Please try again later.",
    "timeout": "Request timed out. Please try again with a shorter query.",
    "unexpected_error": "An unexpected error occurred. Please try again later.",
    "empty_query": "Your query is too short or empty. Please provide more details.",
    "no_encoder_facts": "We could not extract relevant legal information from your query.",
    "decoder_bypassed": "We could not verify the response against the extracted facts.",
    # RAG refusal reasons (Phase R5)
    "rag_retrieval_failed": "We could not find relevant legal documents for your query.",
    "rag_validation_failed": "The retrieved evidence did not meet our quality standards.",
    "rag_context_failed": "We could not assemble sufficient legal context for your query.",
    "insufficient_evidence": "Insufficient legal evidence found to answer your query.",
    "statute_mismatch": "The query references a statute that does not match the available evidence.",
    "section_mismatch": "The requested section was not found in our legal database.",
    "no_valid_chunks": "No valid legal documents matched your query criteria.",
    "missing_citation": "The evidence lacks required citation information.",
    "no_citations_in_answer": "The generated answer did not properly cite the evidence.",
    "hallucinated_law": "The response referenced laws not present in the evidence.",
    # Phase P3: Backpressure refusal reasons
    "server_busy": "Server is at capacity. Please try again in a few moments.",
    "latency_budget_exceeded": "Request took too long to process. Please try again with a simpler query.",
    # Phase P6: Kill switch refusal reasons
    "encoder_disabled": "Encoder is currently disabled. Please contact support.",
    "rag_disabled": "RAG retrieval is currently disabled. Please contact support.",
    "decoder_disabled": "Decoder is currently disabled. Please contact support.",
    # Phase P8: Degraded mode refusal reasons
    "degraded_encoder_only": "System is in degraded mode. Only encoding is available.",
    "degraded_rag_only": "System is in degraded mode. Only RAG retrieval is available.",
    "system_failed": "System is in failed state. Please contact support.",
    # Phase P9: Cost & abuse safety refusal reasons
    "token_budget_exceeded": "Request exceeds token budget. Please shorten your query.",
    "daily_budget_exceeded": "Daily token budget exceeded. Please try again tomorrow.",
    "abuse_detected": "Potential abuse detected. Request blocked.",
}

def _make_refusal(reason: str, encoder_result: Optional[Dict] = None, trace_id: Optional[str] = None) -> Dict[str, Any]:
    """
    Create a structured refusal response.
    
    Phase P6: Always includes config_hash and trace_id.
    """
    response = {
        "status": "refused",
        "reason": reason,
        "message": REFUSAL_MESSAGES.get(reason, REFUSAL_MESSAGES["unexpected_error"]),
        "config_hash": get_config_hash(),
    }
    
    if trace_id:
        response["trace_id"] = trace_id
    
    if encoder_result:
        response["encoder_result"] = encoder_result
    
    return response

# ─────────────────────────────────────────────
# PHASE 3: Production Logging
# ─────────────────────────────────────────────
def _utc_now_iso() -> str:
    return datetime.utcnow().replace(microsecond=0).isoformat() + "Z"

def _log_production_request(
    request_id: str,
    query: str,
    selected_encoder: Optional[str],
    selected_decoder: Optional[str],
    routing_score: Optional[float],
    status: str,
    refusal_reason: Optional[str],
    latency_ms: int
) -> None:
    """Log every request as structured JSON (append-only)."""
    entry = {
        "request_id": request_id,
        "timestamp": _utc_now_iso(),
        "query": query[:500],  # Truncate for safety
        "selected_encoder": selected_encoder,
        "selected_decoder": selected_decoder,
        "routing_score": routing_score,
        "status": status,
        "refusal_reason": refusal_reason,
        "latency_ms": latency_ms
    }
    try:
        with open(PRODUCTION_LOG_FILE, "a", encoding="utf-8") as f:
            f.write(json.dumps(entry, ensure_ascii=False) + "\n")
    except Exception as e:
        logger.error(f"Failed to write production log: {e}")


def _require_hf_token() -> str:
    token = os.environ.get("HF_TOKEN")
    if not token:
        raise HTTPException(
            status_code=500,
            detail="HF_TOKEN environment variable is required.",
        )
    return token


# ─────────────────────────────────────────────
# PHASE 3: HF Inference with Retry Logic (DEPRECATED)
# ─────────────────────────────────────────────
# NOTE: HF Inference API is deprecated. Use local models instead.
# These functions are kept as stubs for backward compatibility but
# should NOT be called in production. All inference goes through
# LocalModelRegistry.
# ─────────────────────────────────────────────

class HFInferenceError(Exception):
    """Custom exception for inference errors (local or API)."""
    def __init__(self, reason: str, details: str = ""):
        self.reason = reason
        self.details = details
        super().__init__(f"{reason}: {details}")


def _hf_infer_with_retry(model_id: str, payload: Dict[str, Any], timeout_s: int = 60) -> Any:
    """
    DEPRECATED: HF Inference API call.
    
    This function is deprecated and should not be called.
    All inference should go through LocalModelRegistry.
    
    Raises:
        HFInferenceError: Always raises - use local models instead
    """
    # DEPRECATED - Do not use HF API
    raise HFInferenceError(
        "encoder_failed",
        "HF Inference API is deprecated. Load local models when GPU is available."
    )
    
    # Original code kept for reference but unreachable:
    # token = _require_hf_token()
    # url = f"https://router.huggingface.co/hf-inference/models/{model_id}"
    # ...


def _hf_infer_with_retry_DEPRECATED(model_id: str, payload: Dict[str, Any], timeout_s: int = 60) -> Any:
    """
    DEPRECATED: Original HF Inference API implementation.
    Kept for reference only - DO NOT USE.
    """
    token = _require_hf_token()
    url = f"https://router.huggingface.co/hf-inference/models/{model_id}"
    headers = {
        "Authorization": f"Bearer {token}",
        "Content-Type": "application/json",
    }
    
    last_error = None
    for attempt in range(MAX_RETRIES + 1):
        try:
            resp = requests.post(url, headers=headers, json=payload, timeout=timeout_s)
            
            # Rate limited - retry once
            if resp.status_code == 429:
                if attempt < MAX_RETRIES:
                    time.sleep(RETRY_DELAY_MS / 1000)
                    continue
                raise HFInferenceError("rate_limited", "Rate limit exceeded after retry")
            
            # Service unavailable - retry once
            if resp.status_code in (502, 503, 504):
                if attempt < MAX_RETRIES:
                    time.sleep(RETRY_DELAY_MS / 1000)
                    continue
                raise HFInferenceError("upstream_unavailable", f"HF API returned {resp.status_code}")
            
            # Other errors - don't retry
            if resp.status_code != 200:
                raise HFInferenceError("upstream_unavailable", f"HF API error: {resp.status_code} - {resp.text[:500]}")
            
            # Parse JSON
            try:
                return resp.json()
            except Exception:
                raise HFInferenceError("upstream_unavailable", "HF returned non-JSON response")
                
        except requests.exceptions.Timeout:
            if attempt < MAX_RETRIES:
                continue
            raise HFInferenceError("timeout", "Request timed out after retry")
        except requests.exceptions.RequestException as e:
            last_error = e
            if attempt < MAX_RETRIES:
                time.sleep(RETRY_DELAY_MS / 1000)
                continue
            raise HFInferenceError("upstream_unavailable", str(e))
    
    raise HFInferenceError("unexpected_error", str(last_error) if last_error else "Unknown error")


def _extract_sections(text: str) -> List[str]:
    matches = re.findall(r"\bSection\s+\d+[A-Za-z]?\b", text, flags=re.IGNORECASE)
    seen, out = set(), []
    for m in matches:
        s = "Section " + re.sub(r"(?i)^section\s+", "", m).strip()
        if s not in seen:
            seen.add(s)
            out.append(s)
    return out


def _run_encoder(model_id: str, text: str) -> Dict[str, Any]:
    """
    Run encoder with error handling.
    
    Uses local model if loaded, otherwise raises encoder_failed.
    
    PHASE P2: Handles EncoderExecutionError for fault containment.
    Server NEVER crashes on encoder failure.
    """
    # Check if local model is loaded
    if MODEL_REGISTRY.is_loaded(model_id):
        # Use local model with Phase P2 isolation
        try:
            result = run_local_encoder(MODEL_REGISTRY, model_id, text)
            return {
                "encoder_model": model_id,
                "facts": result["facts"],
            }
        except EncoderExecutionError as e:
            # Phase P2: Specific encoder execution failure
            logger.error(f"Encoder execution failed: {e.reason} - {e.details}")
            raise HFInferenceError("encoder_failed", e.reason)
        except RuntimeError as e:
            # Model not loaded or other runtime error
            logger.error(f"Encoder runtime error: {e}")
            raise HFInferenceError("encoder_failed", str(e))
        except Exception as e:
            # Catch-all: NEVER crash server
            logger.error(f"Unexpected encoder error: {e}")
            raise HFInferenceError("encoder_failed", "unexpected_error")
    
    # Model not loaded - raise error with helpful message
    raise HFInferenceError(
        "encoder_failed",
        f"Local model not loaded: {model_id}. Load manually when GPU is available."
    )


def _facts_to_block(facts: Dict[str, Any]) -> str:
    lines = ["ENCODER_FACTS_START", "Sections:"]
    for s in facts.get("sections", []) or ["(none)"]:
        lines.append(f"- {s}")
    lines.append("Entities:")
    for e in facts.get("entities", []) or ["(none)"]:
        lines.append(f"- {e}")
    lines.append("ENCODER_FACTS_END")
    return "\n".join(lines)


def _output_mentions_facts(output: str, facts: Dict[str, Any]) -> bool:
    out = output.lower()
    for s in facts.get("sections", []):
        if s.lower() in out:
            return True
    for e in facts.get("entities", []):
        t = e.get("text", "")
        if len(t) > 3 and t.lower() in out:
            return True
    return False


def _run_decoder(model_id: str, query: str, facts_block: str) -> Dict[str, Any]:
    """Run decoder with error handling."""
    prompt = (
        f"Legal Query: {query}\n\n"
        f"Relevant Facts:\n{facts_block}\n\n"
        f"Based on the above facts, provide a legal answer."
    )

    raw = _hf_infer_with_retry(
        model_id,
        {
            "inputs": prompt,
            "parameters": {
                "max_length": 256,
            },
        },
    )

    text = ""
    if isinstance(raw, list) and raw:
        # BART returns summary_text, not generated_text
        text = raw[0].get("summary_text", "") or raw[0].get("generated_text", "")
    elif isinstance(raw, dict):
        text = raw.get("summary_text", "") or raw.get("generated_text", "")

    text = text.strip()

    if not text or text.upper().startswith("REFUSE"):
        return {"status": "refused", "output": "REFUSE: Missing facts."}

    return {"status": "success", "output": text}


# ─────────────────────────────────────────────
# Router
# ─────────────────────────────────────────────
router = MoERouter()

# ─────────────────────────────────────────────
# PHASE R5: RAG Components (Lazy Init)
# ─────────────────────────────────────────────
_rag_retriever: Optional[LegalRetriever] = None
_rag_validator: Optional[RetrievalValidator] = None
_rag_assembler: Optional[ContextAssembler] = None

RAG_MOE_LOG_FILE = Path("logs/rag_moe_pipeline.jsonl")
RAG_MOE_LOG_FILE.parent.mkdir(parents=True, exist_ok=True)

def _get_rag_retriever() -> LegalRetriever:
    """Get or initialize RAG retriever."""
    global _rag_retriever
    if _rag_retriever is None:
        _rag_retriever = LegalRetriever()
        _rag_retriever.initialize(index_dense=True)
    return _rag_retriever

def _get_rag_validator() -> RetrievalValidator:
    """Get or initialize RAG validator."""
    global _rag_validator
    if _rag_validator is None:
        _rag_validator = RetrievalValidator()
    return _rag_validator

def _get_rag_assembler() -> ContextAssembler:
    """Get or initialize RAG context assembler."""
    global _rag_assembler
    if _rag_assembler is None:
        _rag_assembler = ContextAssembler(budget_config=BudgetConfig(max_tokens=2500))
    return _rag_assembler

# Phase R6b: Post-generation verifier
_postgen_verifier: Optional[PostGenerationVerifier] = None

def _get_postgen_verifier() -> PostGenerationVerifier:
    """Get or initialize post-generation verifier."""
    global _postgen_verifier
    if _postgen_verifier is None:
        _postgen_verifier = PostGenerationVerifier()
    return _postgen_verifier

# Phase R7: Replay store and drift logger
_replay_store: Optional[ReplayStore] = None
_drift_logger: Optional[DriftSignalLogger] = None

def _get_replay_store() -> ReplayStore:
    """Get or initialize replay store."""
    global _replay_store
    if _replay_store is None:
        _replay_store = ReplayStore()
    return _replay_store

def _get_drift_logger() -> DriftSignalLogger:
    """Get or initialize drift signal logger."""
    global _drift_logger
    if _drift_logger is None:
        _drift_logger = DriftSignalLogger()
    return _drift_logger

def _log_rag_moe_pipeline(
    request_id: str,
    query: str,
    encoder: Optional[str],
    decoder: Optional[str],
    retrieved_chunks: int,
    accepted_chunks: int,
    used_chunks: int,
    refusal_reason: Optional[str],
    latency_ms: int,
    status: str,
) -> None:
    """Log RAG+MoE pipeline execution."""
    entry = {
        "request_id": request_id,
        "timestamp": _utc_now_iso(),
        "query": query[:500],
        "encoder": encoder,
        "decoder": decoder,
        "retrieved_chunks": retrieved_chunks,
        "accepted_chunks": accepted_chunks,
        "used_chunks": used_chunks,
        "refusal_reason": refusal_reason,
        "latency_ms": latency_ms,
        "status": status,
    }
    try:
        with open(RAG_MOE_LOG_FILE, "a", encoding="utf-8") as f:
            f.write(json.dumps(entry, ensure_ascii=False) + "\n")
    except Exception as e:
        logger.error(f"Failed to write RAG+MoE log: {e}")

def _build_rag_decoder_prompt(query: str, evidence_block: str) -> str:
    """
    Build the MANDATORY decoder prompt with evidence.
    
    The decoder MUST answer ONLY using the evidence.
    """
    return f"""You are a legal assistant.
You MUST answer ONLY using the evidence below.
If the evidence is insufficient, output exactly:
REFUSE: Insufficient legal evidence.

{evidence_block}

Question: {query}
Answer:"""

def _extract_citations_from_answer(answer: str) -> List[str]:
    """Extract citation markers [1], [2], etc. from answer."""
    return re.findall(r'\[(\d+)\]', answer)

def _validate_answer_citations(
    answer: str,
    evidence_block: str,
    used_chunks: List[Dict],
) -> tuple[bool, Optional[str]]:
    """
    Validate that answer only references evidence.
    
    Returns:
        (is_valid, refusal_reason)
    """
    # Check for citation markers
    citations = _extract_citations_from_answer(answer)
    if not citations:
        return False, "no_citations_in_answer"
    
    # Check that cited indices exist in evidence
    max_citation = len(used_chunks)
    for cit in citations:
        if int(cit) > max_citation or int(cit) < 1:
            return False, "hallucinated_law"
    
    # Check for sections/acts not in evidence
    answer_lower = answer.lower()
    evidence_lower = evidence_block.lower()
    
    # Extract sections mentioned in answer
    answer_sections = set(re.findall(r'section\s+(\d+[a-z]?)', answer_lower))
    evidence_sections = set(re.findall(r'section\s+(\d+[a-z]?)', evidence_lower))
    
    # If answer mentions sections not in evidence, refuse
    for sec in answer_sections:
        if sec not in evidence_sections:
            return False, "hallucinated_law"
    
    return True, None

class QueryRequest(BaseModel):
    query: str


@app.get("/health")
def health():
    """
    Basic health check endpoint.
    
    Phase P6: Includes config_hash and feature flags.
    """
    return {
        "status": "ok",
        "config_hash": get_config_hash(),
        "features": get_feature_flags(),
    }


@app.get("/health/gpu")
def health_gpu():
    """
    Phase P4: GPU Health Endpoint
    
    Returns GPU health information including:
    - cuda_available (bool)
    - total_vram_gb (float | null)
    - free_vram_gb (float | null)
    - encoder_loaded (bool)
    - decoder_loaded (bool)
    - model_state (string)
    - timestamp (ISO8601)
    
    This endpoint:
    - Never loads models
    - Never allocates GPU memory
    - Never throws uncaught exceptions
    """
    try:
        from datetime import datetime
        
        # Get GPU info (safe, no allocation)
        gpu_info = get_gpu_health_info()
        
        # Get model state from registry
        model_state = MODEL_REGISTRY.get_state()
        warmup = MODEL_REGISTRY.get_warmup_telemetry()
        
        # Check which models are loaded
        encoder_loaded = MODEL_REGISTRY._encoder_id is not None and MODEL_REGISTRY.is_loaded(MODEL_REGISTRY._encoder_id)
        decoder_loaded = MODEL_REGISTRY._decoder_id is not None and MODEL_REGISTRY.is_loaded(MODEL_REGISTRY._decoder_id)
        
        return {
            "cuda_available": gpu_info["cuda_available"],
            "total_vram_gb": gpu_info["total_vram_gb"],
            "free_vram_gb": gpu_info["free_vram_gb"],
            "encoder_loaded": encoder_loaded,
            "decoder_loaded": decoder_loaded,
            "model_state": model_state.value,
            "warmup": {
                "encoder_warmup_ms": warmup.encoder_warmup_ms,
                "decoder_warmup_ms": warmup.decoder_warmup_ms,
                "first_token_latency_ms": warmup.first_token_latency_ms,
                "encoder_warmed_up": warmup.encoder_warmed_up,
                "decoder_warmed_up": warmup.decoder_warmed_up,
            },
            # Phase P6: Config hash and feature flags
            "config_hash": get_config_hash(),
            "features": get_feature_flags(),
            "timestamp": datetime.utcnow().isoformat(),
        }
    except Exception as e:
        # Never throw uncaught exceptions
        logger.error(f"GPU health check failed: {e}")
        from datetime import datetime
        return {
            "cuda_available": False,
            "total_vram_gb": None,
            "free_vram_gb": None,
            "encoder_loaded": False,
            "decoder_loaded": False,
            "model_state": "unknown",
            "warmup": None,
            "timestamp": datetime.utcnow().isoformat(),
            "error": str(e),
        }


# ─────────────────────────────────────────────
# PHASE P10: OPERATIONAL ENDPOINTS
# ─────────────────────────────────────────────

@app.get("/health/full")
def health_full():
    """
    Phase P10: Full health check endpoint.
    
    Returns comprehensive system state including:
    - config_hash
    - model_state
    - gpu_memory snapshot
    - warmup telemetry
    - feature flags
    - concurrency usage
    - canary mode status
    """
    try:
        gpu_info = get_gpu_health_info()
        memory_snapshot = get_gpu_memory_snapshot()
        model_state = MODEL_REGISTRY.get_state()
        warmup = MODEL_REGISTRY.get_warmup_telemetry()
        
        encoder_loaded = MODEL_REGISTRY._encoder_id is not None and MODEL_REGISTRY.is_loaded(MODEL_REGISTRY._encoder_id)
        decoder_loaded = MODEL_REGISTRY._decoder_id is not None and MODEL_REGISTRY.is_loaded(MODEL_REGISTRY._decoder_id)
        
        return {
            "status": "ok",
            "config_hash": get_config_hash(),
            "model_state": model_state.value,
            "gpu": {
                "cuda_available": gpu_info["cuda_available"],
                "total_vram_gb": gpu_info["total_vram_gb"],
                "free_vram_gb": gpu_info["free_vram_gb"],
            },
            "gpu_memory": memory_snapshot.to_dict(),
            "models": {
                "encoder_loaded": encoder_loaded,
                "decoder_loaded": decoder_loaded,
            },
            "warmup": {
                "encoder_warmup_ms": warmup.encoder_warmup_ms,
                "decoder_warmup_ms": warmup.decoder_warmup_ms,
                "first_token_latency_ms": warmup.first_token_latency_ms,
                "encoder_warmed_up": warmup.encoder_warmed_up,
                "decoder_warmed_up": warmup.decoder_warmed_up,
            },
            "features": get_feature_flags(),
            "canary_mode": is_canary_mode(),
            "concurrency": {
                "max_concurrent": MAX_CONCURRENT_REQUESTS,
                "semaphore_value": _request_semaphore._value,
            },
            "timestamp": datetime.utcnow().isoformat(),
        }
    except Exception as e:
        logger.error(f"Full health check failed: {e}")
        return {
            "status": "error",
            "error": str(e),
            "timestamp": datetime.utcnow().isoformat(),
        }


@app.get("/ops/state")
def ops_state():
    """
    Phase P10: Get current operational state.
    """
    return {
        "model_state": MODEL_REGISTRY.get_state().value,
        "failure_reason": getattr(MODEL_REGISTRY, '_failure_reason', None),
        "config_hash": get_config_hash(),
        "features": get_feature_flags(),
        "canary_mode": is_canary_mode(),
        "ops_override_allowed": is_ops_override_allowed(),
        "timestamp": datetime.utcnow().isoformat(),
    }


class ForceStateRequest(BaseModel):
    state: str  # "ready", "degraded", "failed"
    reason: Optional[str] = None


@app.post("/ops/force-state")
def ops_force_state(req: ForceStateRequest):
    """
    Phase P10: Force model state (requires ALLOW_OPS_OVERRIDE=true).
    
    Allowed states: ready, degraded, degraded_encoder_only, degraded_rag_only, failed
    """
    if not is_ops_override_allowed():
        return {
            "status": "error",
            "message": "Ops override not allowed. Set ALLOW_OPS_OVERRIDE=true",
        }
    
    valid_states = {
        "ready": ModelState.READY,
        "degraded": ModelState.DEGRADED,
        "degraded_encoder_only": ModelState.DEGRADED_ENCODER_ONLY,
        "degraded_rag_only": ModelState.DEGRADED_RAG_ONLY,
        "failed": ModelState.FAILED,
    }
    
    if req.state not in valid_states:
        return {
            "status": "error",
            "message": f"Invalid state. Valid: {list(valid_states.keys())}",
        }
    
    old_state = MODEL_REGISTRY.get_state()
    MODEL_REGISTRY._state = valid_states[req.state]
    
    if req.state == "failed" and req.reason:
        MODEL_REGISTRY._failure_reason = req.reason
    
    logger.warning(f"[OPS] State forced: {old_state.value} -> {req.state} (reason: {req.reason})")
    
    return {
        "status": "ok",
        "old_state": old_state.value,
        "new_state": req.state,
        "reason": req.reason,
        "timestamp": datetime.utcnow().isoformat(),
    }


@app.post("/moe-test")
def moe_test(req: QueryRequest):
    r = router.route(req.query, task_hint="qa", role="decoder")[0]
    return r


@app.post("/moe-encode")
def moe_encode(req: QueryRequest):
    enc = router.route(req.query, task_hint="ner", role="encoder")[0]
    out = _run_encoder(enc["model_id"], req.query)
    return out


@app.post("/moe-generate")
async def moe_generate(req: QueryRequest, audit: bool = False):
    """
    Phase R7: MoE + RAG Integrated Generation with Audit & Trace
    
    PIPELINE ORDER:
    1. Encoder (extract sections/entities)
    2. RAG Retrieval (BM25 + Dense)
    3. RAG Validation (statute, section, threshold)
    4. Context Assembly (evidence block)
    5. Decoder (with evidence-only prompt)
    6. Post-generation enforcement (citation check)
    7. Post-generation verification (R6b)
    
    PHASE R7 ADDITIONS:
    - trace_id attached to all stages
    - Replay artifact persisted for every request
    - Decision provenance in all responses
    - Audit mode (?audit=true) returns extra fields
    
    PHASE P3 ADDITIONS:
    - Concurrency limit (MAX_CONCURRENT_REQUESTS)
    - Latency budget (TOTAL_REQUEST_BUDGET_MS)
    - Per-stage timing in logs
    
    CRITICAL INVARIANTS:
    - Decoder NEVER sees raw query alone
    - Decoder ONLY sees assembled context
    - If RAG refuses → decoder NOT called
    - All refusals are server-side enforced
    """
    # ─────────────────────────────────────────────
    # PHASE P3: CONCURRENCY CONTROL (FAIL-FAST)
    # ─────────────────────────────────────────────
    # Try to acquire semaphore without waiting
    if not _request_semaphore.locked() or _request_semaphore._value > 0:
        pass  # Will acquire below
    else:
        # At capacity - fail fast, no queuing
        return _make_refusal("server_busy")
    
    # Acquire semaphore (should not block given check above)
    acquired = False
    try:
        # Use try_acquire pattern
        try:
            await asyncio.wait_for(_request_semaphore.acquire(), timeout=0.01)
            acquired = True
        except asyncio.TimeoutError:
            return _make_refusal("server_busy")
    except Exception:
        return _make_refusal("server_busy")
    
    # Initialize latency tracker
    latency = LatencyTracker(TOTAL_REQUEST_BUDGET_MS)
    
    # ─────────────────────────────────────────────
    # PHASE R7: Generate trace_id at request entry
    # ─────────────────────────────────────────────
    trace_id = generate_trace_id()
    start_time = time.time()
    query = req.query.strip() if req.query else ""
    
    # Initialize replay artifact
    # Phase P6: Include config_hash and feature_flags
    # Phase P8: Include canary_mode
    replay = ReplayArtifact(
        trace_id=trace_id,
        timestamp=datetime.utcnow().isoformat(),
        query=query,
        config_hash=get_config_hash(),
        feature_flags=get_feature_flags(),
        canary_mode=is_canary_mode(),
    )
    
    # Phase P9: Token accounting
    token_accounting = {"input_tokens": 0, "output_tokens": 0, "total": 0}
    
    # Initialize provenance builder
    provenance = ProvenanceBuilder()
    
    selected_encoder = None
    selected_decoder = None
    routing_score = None
    enc_out = None
    retrieved_count = 0
    accepted_count = 0
    used_count = 0
    evidence_block = ""
    decoder_prompt = ""
    decoder_output = ""
    raw_chunks_data = []
    validated_chunks_data = []
    rejected_chunks_data = []
    
    # Phase P5: GPU memory telemetry
    encoder_gpu_memory = None
    decoder_gpu_memory = None
    
    def _finalize_and_return(response: Dict[str, Any], refusal_reason: Optional[str] = None) -> Dict[str, Any]:
        """Finalize replay artifact and return response."""
        nonlocal replay, acquired
        
        # ─────────────────────────────────────────────
        # PHASE P3: ALWAYS RELEASE SEMAPHORE
        # ─────────────────────────────────────────────
        if acquired:
            _request_semaphore.release()
            acquired = False
        
        latency_ms = int((time.time() - start_time) * 1000)
        replay.latency_ms = latency_ms
        
        # Build final response trace
        replay.final_response = FinalResponse(
            status=response.get("status", "unknown"),
            answer_or_message=response.get("answer", response.get("message", "")),
            citations=response.get("citations", []),
        )
        
        # Build decision provenance
        if response.get("status") == "success":
            replay.decision_provenance = provenance.build_success()
        else:
            replay.decision_provenance = provenance.build_refusal(refusal_reason or "unknown")
        
        # Phase P5: Add GPU memory telemetry to replay artifact
        if encoder_gpu_memory or decoder_gpu_memory:
            replay.gpu_memory = GPUMemoryTrace(
                encoder=encoder_gpu_memory,
                decoder=decoder_gpu_memory,
            )
        
        # Phase P9: Add token accounting to replay artifact
        replay.token_accounting = token_accounting
        
        # Persist replay artifact
        try:
            _get_replay_store().save(replay)
        except Exception as e:
            logger.error(f"Failed to save replay artifact: {e}")
        
        # Add trace_id and provenance to response
        response["trace_id"] = trace_id
        response["decision_provenance"] = replay.decision_provenance.to_dict()
        
        # Phase P3: Add latency timings to response
        response["latency"] = latency.get_timings()
        
        # Add extra fields in audit mode
        if audit:
            response["evidence_block"] = evidence_block
        
        return response
    
    try:
        # ─────────────────────────────────────────────
        # PHASE P6: KILL SWITCH CHECKS (FAIL-FAST)
        # ─────────────────────────────────────────────
        if not is_encoder_enabled():
            log_with_trace(RAG_MOE_LOG_FILE, trace_id, "request", "refused", "encoder_disabled")
            provenance.add_rule("encoder_disabled")
            return _finalize_and_return(_make_refusal("encoder_disabled", trace_id=trace_id), "encoder_disabled")
        
        if not is_rag_enabled():
            log_with_trace(RAG_MOE_LOG_FILE, trace_id, "request", "refused", "rag_disabled")
            provenance.add_rule("rag_disabled")
            return _finalize_and_return(_make_refusal("rag_disabled", trace_id=trace_id), "rag_disabled")
        
        if not is_decoder_enabled():
            log_with_trace(RAG_MOE_LOG_FILE, trace_id, "request", "refused", "decoder_disabled")
            provenance.add_rule("decoder_disabled")
            return _finalize_and_return(_make_refusal("decoder_disabled", trace_id=trace_id), "decoder_disabled")
        
        # ─────────────────────────────────────────────
        # REFUSAL CHECK 1: Empty or ambiguous query
        # ─────────────────────────────────────────────
        if not query or len(query) < 10:
            log_with_trace(RAG_MOE_LOG_FILE, trace_id, "request", "refused", "empty_query")
            provenance.add_rule("query_too_short")
            return _finalize_and_return(_make_refusal("empty_query", trace_id=trace_id), "empty_query")
        
        provenance.add_rule("query_valid")
        
        # ─────────────────────────────────────────────
        # STEP 1: ENCODER ALWAYS RUNS FIRST
        # ─────────────────────────────────────────────
        # Phase P3: Check budget before encoder
        try:
            latency.check_budget("encoder")
        except RuntimeError:
            log_with_trace(RAG_MOE_LOG_FILE, trace_id, "encoder", "refused", "latency_budget_exceeded")
            provenance.add_rule("budget_exceeded_before_encoder")
            return _finalize_and_return(_make_refusal("latency_budget_exceeded", trace_id=trace_id), "latency_budget_exceeded")
        
        latency.start_stage("encoder")
        
        enc_routes = router.route(query, task_hint="ner", role="encoder")
        if not enc_routes:
            latency.end_stage()
            log_with_trace(RAG_MOE_LOG_FILE, trace_id, "encoder", "refused", "encoder_failed")
            provenance.add_rule("no_encoder_route")
            return _finalize_and_return(_make_refusal("encoder_failed"), "encoder_failed")
        
        enc = enc_routes[0]
        selected_encoder = enc["model_id"]
        routing_score = enc.get("score")
        
        try:
            enc_out = _run_encoder(enc["model_id"], query)
            # Phase P5: Extract GPU memory from encoder result
            encoder_gpu_memory = enc_out.get("gpu_memory")
        except HFInferenceError as e:
            latency.end_stage()
            log_with_trace(RAG_MOE_LOG_FILE, trace_id, "encoder", "refused", e.reason)
            provenance.add_rule("encoder_api_error")
            return _finalize_and_return(_make_refusal(e.reason), e.reason)
        
        latency.end_stage()
        facts = enc_out["facts"]
        
        # Add encoder trace
        replay.encoder = EncoderTrace(model=selected_encoder, facts=facts)
        provenance.add_rule("encoder_success")
        log_with_trace(RAG_MOE_LOG_FILE, trace_id, "encoder", "success")
        
        # ─────────────────────────────────────────────
        # STEP 2: RAG RETRIEVAL
        # ─────────────────────────────────────────────
        # Phase P3: Check budget before retrieval
        try:
            latency.check_budget("retrieval")
        except RuntimeError:
            log_with_trace(RAG_MOE_LOG_FILE, trace_id, "retrieval", "refused", "latency_budget_exceeded")
            provenance.add_rule("budget_exceeded_before_retrieval")
            return _finalize_and_return(_make_refusal("latency_budget_exceeded"), "latency_budget_exceeded")
        
        latency.start_stage("retrieval")
        
        try:
            retriever = _get_rag_retriever()
            retrieved_chunks = retriever.retrieve(query, top_k=10, method="fused")
            retrieved_count = len(retrieved_chunks)
            
            # Store raw chunks for replay
            raw_chunks_data = [
                {'chunk_id': c.chunk_id, 'text': c.text[:200], 'section': c.section, 'act': c.act, 'score': c.score}
                for c in retrieved_chunks
            ]
        except Exception as e:
            logger.error(f"RAG retrieval failed: {e}")
            latency.end_stage()
            log_with_trace(RAG_MOE_LOG_FILE, trace_id, "retrieval", "refused", "rag_retrieval_failed")
            provenance.add_rule("retrieval_error")
            return _finalize_and_return(_make_refusal("rag_retrieval_failed", enc_out), "rag_retrieval_failed")
        
        latency.end_stage()
        
        if not retrieved_chunks:
            log_with_trace(RAG_MOE_LOG_FILE, trace_id, "retrieval", "refused", "insufficient_evidence")
            provenance.add_rule("no_chunks_retrieved")
            return _finalize_and_return(_make_refusal("insufficient_evidence", enc_out), "insufficient_evidence")
        
        provenance.add_rule("retrieval_success")
        log_with_trace(RAG_MOE_LOG_FILE, trace_id, "retrieval", "success", extra={"chunk_count": retrieved_count})
        
        # ─────────────────────────────────────────────
        # STEP 3: RAG VALIDATION (R3)
        # ─────────────────────────────────────────────
        # Phase P3: Check budget before validation
        try:
            latency.check_budget("validation")
        except RuntimeError:
            log_with_trace(RAG_MOE_LOG_FILE, trace_id, "validation", "refused", "latency_budget_exceeded")
            provenance.add_rule("budget_exceeded_before_validation")
            return _finalize_and_return(_make_refusal("latency_budget_exceeded"), "latency_budget_exceeded")
        
        latency.start_stage("validation")
        
        try:
            validator = _get_rag_validator()
            validation_result = validator.validate(
                query=query,
                retrieved_chunks=[
                    {
                        'chunk_id': c.chunk_id,
                        'text': c.text,
                        'section': c.section,
                        'act': c.act,
                        'score': c.score,
                        'doc_type': c.doc_type,
                        'year': c.year,
                        'court': c.court,
                        'citation': c.citation,
                    }
                    for c in retrieved_chunks
                ],
            )
        except Exception as e:
            logger.error(f"RAG validation failed: {e}")
            latency.end_stage()
            log_with_trace(RAG_MOE_LOG_FILE, trace_id, "validation", "refused", "rag_validation_failed")
            provenance.add_rule("validation_error")
            return _finalize_and_return(_make_refusal("rag_validation_failed", enc_out), "rag_validation_failed")
        
        latency.end_stage()
        
        # Check validation result
        if validation_result.status.value == "refuse":
            refusal_reason = validation_result.refusal_reason.value if validation_result.refusal_reason else "rag_validation_failed"
            log_with_trace(RAG_MOE_LOG_FILE, trace_id, "validation", "refused", refusal_reason)
            provenance.add_rule("validation_refused")
            
            # Store retrieval trace even on refusal
            validated_chunks_data = []
            rejected_chunks_data = [{'chunk_id': c.chunk_id, 'section': c.section, 'act': c.act} for c in validation_result.rejected_chunks] if hasattr(validation_result, 'rejected_chunks') else []
            replay.retrieval = RetrievalTrace(raw_chunks=raw_chunks_data, validated_chunks=validated_chunks_data, rejected_chunks=rejected_chunks_data)
            
            return _finalize_and_return(_make_refusal(refusal_reason, enc_out), refusal_reason)
        
        accepted_count = len(validation_result.accepted_chunks)
        provenance.add_rule("validation_passed")
        log_with_trace(RAG_MOE_LOG_FILE, trace_id, "validation", "success", extra={"accepted_count": accepted_count})
        
        # Store validated chunks for replay
        validated_chunks_data = [{'chunk_id': c.chunk_id, 'text': c.text[:200], 'section': c.section, 'act': c.act} for c in validation_result.accepted_chunks]
        rejected_chunks_data = [{'chunk_id': c.chunk_id, 'section': c.section, 'act': c.act} for c in validation_result.rejected_chunks] if hasattr(validation_result, 'rejected_chunks') else []
        replay.retrieval = RetrievalTrace(raw_chunks=raw_chunks_data, validated_chunks=validated_chunks_data, rejected_chunks=rejected_chunks_data)
        
        # ─────────────────────────────────────────────
        # STEP 4: CONTEXT ASSEMBLY (R4)
        # ─────────────────────────────────────────────
        # Phase P3: Check budget before context assembly
        try:
            latency.check_budget("context")
        except RuntimeError:
            log_with_trace(RAG_MOE_LOG_FILE, trace_id, "context", "refused", "latency_budget_exceeded")
            provenance.add_rule("budget_exceeded_before_context")
            return _finalize_and_return(_make_refusal("latency_budget_exceeded"), "latency_budget_exceeded")
        
        latency.start_stage("context")
        
        try:
            assembler = _get_rag_assembler()
            context_result = assembler.assemble(
                query=query,
                validated_chunks=[
                    {
                        'chunk_id': c.chunk_id,
                        'text': c.text,
                        'section': c.section,
                        'act': c.act,
                        'score': c.adjusted_score,
                        'doc_type': 'bare_act',
                        'year': 1860,
                    }
                    for c in validation_result.accepted_chunks
                ],
            )
        except Exception as e:
            logger.error(f"RAG context assembly failed: {e}")
            latency.end_stage()
            log_with_trace(RAG_MOE_LOG_FILE, trace_id, "context", "refused", "rag_context_failed")
            provenance.add_rule("context_error")
            return _finalize_and_return(_make_refusal("rag_context_failed", enc_out), "rag_context_failed")
        
        latency.end_stage()
        
        # Check context result
        if context_result.status.value == "refuse":
            refusal_reason = context_result.refusal_reason.value if context_result.refusal_reason else "rag_context_failed"
            log_with_trace(RAG_MOE_LOG_FILE, trace_id, "context", "refused", refusal_reason)
            provenance.add_rule("context_refused")
            return _finalize_and_return(_make_refusal(refusal_reason, enc_out), refusal_reason)
        
        used_count = len(context_result.used_chunks)
        evidence_block = context_result.context_text
        provenance.add_rule("context_assembled")
        log_with_trace(RAG_MOE_LOG_FILE, trace_id, "context", "success", extra={"used_count": used_count, "token_count": context_result.token_count})
        
        # Store context trace
        used_chunks_data = [{'chunk_id': c.chunk_id, 'section': c.section, 'act': c.act} for c in context_result.used_chunks]
        dropped_chunks_data = [{'chunk_id': c.chunk_id, 'reason': c.reason} for c in context_result.dropped_chunks] if context_result.dropped_chunks else []
        replay.context = ContextTrace(evidence_block=evidence_block, token_count=context_result.token_count, used_chunks=used_chunks_data, dropped_chunks=dropped_chunks_data)
        
        # ─────────────────────────────────────────────
        # STEP 5: DECODER WITH EVIDENCE-ONLY PROMPT
        # ─────────────────────────────────────────────
        # Phase P3: Check budget before decoder
        try:
            latency.check_budget("decoder")
        except RuntimeError:
            log_with_trace(RAG_MOE_LOG_FILE, trace_id, "decoder", "refused", "latency_budget_exceeded")
            provenance.add_rule("budget_exceeded_before_decoder")
            return _finalize_and_return(_make_refusal("latency_budget_exceeded"), "latency_budget_exceeded")
        
        latency.start_stage("decoder")
        
        dec_routes = router.route(query, task_hint="qa", role="decoder")
        if not dec_routes:
            latency.end_stage()
            log_with_trace(RAG_MOE_LOG_FILE, trace_id, "decoder", "refused", "decoder_failed")
            provenance.add_rule("no_decoder_route")
            return _finalize_and_return(_make_refusal("decoder_failed", enc_out), "decoder_failed")
        
        dec = dec_routes[0]
        selected_decoder = dec["model_id"]
        
        # Build the MANDATORY evidence-only prompt
        decoder_prompt = _build_rag_decoder_prompt(query, evidence_block)
        
        # Phase P5: Track decoder GPU memory
        decoder_gpu_memory = None
        
        try:
            # Use local model if loaded (with Phase P2 isolation)
            if MODEL_REGISTRY.is_loaded(dec["model_id"]):
                decoder_result = run_local_decoder(
                    MODEL_REGISTRY,
                    dec["model_id"],
                    decoder_prompt,
                    max_tokens=512,
                )
                # Phase P5: Extract text and gpu_memory from result
                decoder_output = decoder_result["text"]
                decoder_gpu_memory = decoder_result.get("gpu_memory")
            else:
                # Model not loaded - raise error
                raise HFInferenceError(
                    "decoder_failed",
                    f"Local model not loaded: {dec['model_id']}. Load manually when GPU is available."
                )
            
            decoder_output = decoder_output.strip()
            
        except DecoderExecutionError as e:
            # Phase P2: Specific decoder execution failure (timeout, OOM, etc.)
            logger.error(f"Decoder execution failed: {e.reason} - {e.details}")
            latency.end_stage()
            log_with_trace(RAG_MOE_LOG_FILE, trace_id, "decoder", "refused", e.reason)
            provenance.add_rule(f"decoder_{e.reason}")
            return _finalize_and_return(_make_refusal("decoder_failed", enc_out), "decoder_failed")
        except HFInferenceError as e:
            latency.end_stage()
            log_with_trace(RAG_MOE_LOG_FILE, trace_id, "decoder", "refused", e.reason)
            provenance.add_rule("decoder_api_error")
            return _finalize_and_return(_make_refusal(e.reason, enc_out), e.reason)
        except RuntimeError as e:
            # Model not loaded or other runtime error
            logger.error(f"Decoder runtime error: {e}")
            latency.end_stage()
            log_with_trace(RAG_MOE_LOG_FILE, trace_id, "decoder", "refused", "decoder_failed")
            provenance.add_rule("decoder_runtime_error")
            return _finalize_and_return(_make_refusal("decoder_failed", enc_out), "decoder_failed")
        except Exception as e:
            # Catch-all: NEVER crash server (Phase P2 guarantee)
            logger.error(f"Unexpected decoder error: {e}")
            latency.end_stage()
            log_with_trace(RAG_MOE_LOG_FILE, trace_id, "decoder", "refused", "decoder_failed")
            provenance.add_rule("decoder_error")
            return _finalize_and_return(_make_refusal("decoder_failed", enc_out), "decoder_failed")
        
        latency.end_stage()
        
        # Store decoder trace
        replay.decoder = DecoderTrace(model=selected_decoder, prompt=decoder_prompt, raw_output=decoder_output)
        log_with_trace(RAG_MOE_LOG_FILE, trace_id, "decoder", "success")
        
        # ─────────────────────────────────────────────
        # STEP 6: POST-GENERATION ENFORCEMENT
        # ─────────────────────────────────────────────
        
        # Check if decoder refused
        if not decoder_output or decoder_output.upper().startswith("REFUSE"):
            log_with_trace(RAG_MOE_LOG_FILE, trace_id, "postgen", "refused", "decoder_refused")
            provenance.add_rule("decoder_refused")
            replay.post_generation = PostGenTrace(verdict="refuse", violations=["decoder_refused"], extracted_sections=[], extracted_acts=[], extracted_courts=[])
            return _finalize_and_return(_make_refusal("insufficient_evidence", enc_out), "insufficient_evidence")
        
        # ─────────────────────────────────────────────
        # STEP 7: POST-GENERATION VERIFICATION (Phase R6b)
        # ─────────────────────────────────────────────
        # Phase P3: Check budget before post-gen verification
        try:
            latency.check_budget("postgen")
        except RuntimeError:
            log_with_trace(RAG_MOE_LOG_FILE, trace_id, "postgen", "refused", "latency_budget_exceeded")
            provenance.add_rule("budget_exceeded_before_postgen")
            return _finalize_and_return(_make_refusal("latency_budget_exceeded"), "latency_budget_exceeded")
        
        latency.start_stage("postgen")
        
        verifier = _get_postgen_verifier()
        
        evidence_chunks_for_verify = [
            {"chunk_id": c.chunk_id, "section": c.section, "act": c.act}
            for c in context_result.used_chunks
        ]
        
        verification_result = verifier.verify(
            output_text=decoder_output,
            evidence_chunks=evidence_chunks_for_verify,
            evidence_context=evidence_block,
        )
        
        latency.end_stage()
        
        # Log verification with trace_id
        verifier.log_verification(query, selected_decoder, verification_result)
        
        # Store post-gen trace
        violations = [verification_result.refusal_reason] if verification_result.refusal_reason else []
        replay.post_generation = PostGenTrace(
            verdict=verification_result.status.value,
            violations=violations,
            extracted_sections=verification_result.extracted_sections,
            extracted_acts=verification_result.extracted_acts,
            extracted_courts=verification_result.extracted_courts,
        )
        
        # If verification fails, discard model output and refuse
        if verification_result.status == VerificationStatus.REFUSE:
            log_with_trace(RAG_MOE_LOG_FILE, trace_id, "postgen", "refused", verification_result.refusal_reason)
            provenance.add_rule("postgen_verification_failed")
            
            # Log drift signal for hallucinated content
            drift_logger = _get_drift_logger()
            if "hallucinated_section" in (verification_result.refusal_reason or ""):
                section = verification_result.refusal_reason.split(":")[-1] if ":" in verification_result.refusal_reason else "unknown"
                drift_logger.log_unknown_section(trace_id, section, query)
            elif "hallucinated_act" in (verification_result.refusal_reason or ""):
                act = verification_result.refusal_reason.split(":")[-1] if ":" in verification_result.refusal_reason else "unknown"
                drift_logger.log_unknown_act(trace_id, act, query)
            
            return _finalize_and_return({
                "status": "refused",
                "reason": verification_result.refusal_reason,
                "message": verification_result.refusal_message,
            }, verification_result.refusal_reason)
        
        provenance.add_rule("postgen_verification_passed")
        provenance.add_rule("citation_present")
        provenance.add_rule("no_hallucinated_section")
        provenance.add_rule("min_evidence_passed")
        log_with_trace(RAG_MOE_LOG_FILE, trace_id, "postgen", "success")
        
        # ─────────────────────────────────────────────
        # SUCCESS: All invariants satisfied
        # ─────────────────────────────────────────────
        citations = _extract_citations_from_answer(decoder_output)
        
        response = {
            "status": "success",
            "answer": decoder_output,
            "citations": citations,
            "disclaimer": LEGAL_DISCLAIMER,
            "encoder": selected_encoder,
            "decoder": selected_decoder,
            "rag_chunks_used": used_count,
            "moe_controlled": True,
        }
        
        log_with_trace(RAG_MOE_LOG_FILE, trace_id, "complete", "success")
        return _finalize_and_return(response)
        
    except Exception as e:
        # ─────────────────────────────────────────────
        # CATCH-ALL: No silent failures
        # ─────────────────────────────────────────────
        logger.exception(f"Unexpected error in moe_generate: {e}")
        log_with_trace(RAG_MOE_LOG_FILE, trace_id, "error", "error", "unexpected_error")
        provenance.add_rule("unexpected_error")
        return _finalize_and_return(_make_refusal("unexpected_error", enc_out), "unexpected_error")


if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
