"""
Trace & Replay Module

Phase R7: Audit, Trace & Replay

Provides:
- Trace ID generation
- Replay artifact persistence
- Decision provenance tracking
- Drift signal logging

Every request must be traceable end-to-end.
Every answer must be replayable offline.
Every decision must have machine-readable provenance.
"""

import json
import uuid
from dataclasses import dataclass, field, asdict
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Any, Optional


# ─────────────────────────────────────────────
# Trace ID Generation
# ─────────────────────────────────────────────

def generate_trace_id() -> str:
    """Generate a UUID4 trace_id for request tracing."""
    return str(uuid.uuid4())


# ─────────────────────────────────────────────
# Replay Artifact Structure
# ─────────────────────────────────────────────

@dataclass
class EncoderTrace:
    """Encoder execution trace."""
    model: str
    facts: Dict[str, Any]


@dataclass
class RetrievalTrace:
    """RAG retrieval trace."""
    raw_chunks: List[Dict[str, Any]]
    validated_chunks: List[Dict[str, Any]]
    rejected_chunks: List[Dict[str, Any]]


@dataclass
class ContextTrace:
    """Context assembly trace."""
    evidence_block: str
    token_count: int
    used_chunks: List[Dict[str, Any]]
    dropped_chunks: List[Dict[str, Any]]


@dataclass
class DecoderTrace:
    """Decoder execution trace."""
    model: str
    prompt: str
    raw_output: str


@dataclass
class PostGenTrace:
    """Post-generation verification trace."""
    verdict: str  # "pass" or "refuse"
    violations: List[str]
    extracted_sections: List[str]
    extracted_acts: List[str]
    extracted_courts: List[str]


@dataclass
class FinalResponse:
    """Final response trace."""
    status: str
    answer_or_message: str
    citations: List[str]


@dataclass
class DecisionProvenance:
    """
    Machine-readable decision provenance.
    Generated by code, NOT the model.
    """
    answered: bool
    reason: str
    rules_triggered: List[str] = field(default_factory=list)
    
    def to_dict(self) -> Dict[str, Any]:
        return asdict(self)


@dataclass
class GPUMemoryTrace:
    """
    Phase P5: GPU memory telemetry trace.
    
    Captures memory snapshots before/after encoder and decoder execution.
    """
    encoder: Optional[Dict[str, Any]] = None  # {"before": {...}, "after": {...}}
    decoder: Optional[Dict[str, Any]] = None  # {"before": {...}, "after": {...}}


@dataclass
class ReplayArtifact:
    """
    Complete replay artifact for a request.
    
    Must be sufficient to reproduce the answer WITHOUT calling models.
    """
    trace_id: str
    timestamp: str
    query: str
    
    encoder: Optional[EncoderTrace] = None
    retrieval: Optional[RetrievalTrace] = None
    context: Optional[ContextTrace] = None
    decoder: Optional[DecoderTrace] = None
    post_generation: Optional[PostGenTrace] = None
    final_response: Optional[FinalResponse] = None
    decision_provenance: Optional[DecisionProvenance] = None
    
    # Phase P5: GPU memory telemetry
    gpu_memory: Optional[GPUMemoryTrace] = None
    
    # Phase P6: Configuration integrity
    config_hash: Optional[str] = None
    feature_flags: Optional[Dict[str, bool]] = None
    
    # Phase P8: Canary mode
    canary_mode: bool = False
    
    # Phase P9: Token accounting
    token_accounting: Optional[Dict[str, int]] = None  # input_tokens, output_tokens, total
    
    latency_ms: int = 0
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary for JSON serialization."""
        result = {
            "trace_id": self.trace_id,
            "timestamp": self.timestamp,
            "query": self.query,
            "latency_ms": self.latency_ms,
            # Phase P6: Always include config_hash and feature_flags
            "config_hash": self.config_hash,
            "feature_flags": self.feature_flags,
            # Phase P8: Canary mode
            "canary_mode": self.canary_mode,
            # Phase P9: Token accounting
            "token_accounting": self.token_accounting,
        }
        
        if self.encoder:
            result["encoder"] = asdict(self.encoder)
        
        if self.retrieval:
            result["retrieval"] = asdict(self.retrieval)
        
        if self.context:
            result["context"] = asdict(self.context)
        
        if self.decoder:
            result["decoder"] = asdict(self.decoder)
        
        if self.post_generation:
            result["post_generation"] = asdict(self.post_generation)
        
        if self.final_response:
            result["final_response"] = asdict(self.final_response)
        
        if self.decision_provenance:
            result["decision_provenance"] = self.decision_provenance.to_dict()
        
        # Phase P5: GPU memory telemetry
        if self.gpu_memory:
            result["gpu_memory"] = asdict(self.gpu_memory)
        
        return result


# ─────────────────────────────────────────────
# Replay Artifact Persistence
# ─────────────────────────────────────────────

class ReplayStore:
    """Persists replay artifacts to filesystem."""
    
    def __init__(self, base_path: str = "data/replay"):
        self.base_path = Path(base_path)
        self.base_path.mkdir(parents=True, exist_ok=True)
    
    def save(self, artifact: ReplayArtifact) -> Path:
        """
        Save replay artifact to filesystem.
        
        Args:
            artifact: ReplayArtifact to save
            
        Returns:
            Path to saved file
        """
        file_path = self.base_path / f"{artifact.trace_id}.json"
        
        with open(file_path, 'w', encoding='utf-8') as f:
            json.dump(artifact.to_dict(), f, indent=2, ensure_ascii=False)
        
        return file_path
    
    def load(self, trace_id: str) -> Optional[Dict[str, Any]]:
        """
        Load replay artifact from filesystem.
        
        Args:
            trace_id: Trace ID to load
            
        Returns:
            Artifact dictionary or None if not found
        """
        file_path = self.base_path / f"{trace_id}.json"
        
        if not file_path.exists():
            return None
        
        with open(file_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    
    def exists(self, trace_id: str) -> bool:
        """Check if replay artifact exists."""
        return (self.base_path / f"{trace_id}.json").exists()


# ─────────────────────────────────────────────
# Drift Signal Logging
# ─────────────────────────────────────────────

class DriftSignalLogger:
    """
    Logs drift signals for passive monitoring.
    
    Logs (does NOT act on):
    - Unknown sections
    - Unknown acts
    - Repeated refusals on same query pattern
    """
    
    def __init__(self, log_path: str = "logs/drift_signals.jsonl"):
        self.log_path = Path(log_path)
        self.log_path.parent.mkdir(parents=True, exist_ok=True)
    
    def log_unknown_section(self, trace_id: str, section: str, query: str) -> None:
        """Log unknown section reference."""
        self._log({
            "type": "unknown_section",
            "trace_id": trace_id,
            "section": section,
            "query": query[:200],
            "timestamp": datetime.utcnow().isoformat(),
        })
    
    def log_unknown_act(self, trace_id: str, act: str, query: str) -> None:
        """Log unknown act reference."""
        self._log({
            "type": "unknown_act",
            "trace_id": trace_id,
            "act": act,
            "query": query[:200],
            "timestamp": datetime.utcnow().isoformat(),
        })
    
    def log_repeated_refusal(self, trace_id: str, query_pattern: str, refusal_reason: str) -> None:
        """Log repeated refusal pattern."""
        self._log({
            "type": "repeated_refusal",
            "trace_id": trace_id,
            "query_pattern": query_pattern[:200],
            "refusal_reason": refusal_reason,
            "timestamp": datetime.utcnow().isoformat(),
        })
    
    def _log(self, entry: Dict[str, Any]) -> None:
        """Append log entry to file."""
        try:
            with open(self.log_path, 'a', encoding='utf-8') as f:
                f.write(json.dumps(entry, ensure_ascii=False) + '\n')
        except Exception:
            pass  # Passive logging - don't fail on errors


# ─────────────────────────────────────────────
# Decision Provenance Builder
# ─────────────────────────────────────────────

class ProvenanceBuilder:
    """Builds decision provenance from pipeline state."""
    
    def __init__(self):
        self.rules_triggered: List[str] = []
    
    def add_rule(self, rule: str) -> None:
        """Add a triggered rule."""
        if rule not in self.rules_triggered:
            self.rules_triggered.append(rule)
    
    def build_success(self, reason: str = "all_checks_passed") -> DecisionProvenance:
        """Build provenance for successful answer."""
        return DecisionProvenance(
            answered=True,
            reason=reason,
            rules_triggered=self.rules_triggered.copy(),
        )
    
    def build_refusal(self, reason: str) -> DecisionProvenance:
        """Build provenance for refusal."""
        return DecisionProvenance(
            answered=False,
            reason=reason,
            rules_triggered=self.rules_triggered.copy(),
        )


# ─────────────────────────────────────────────
# Trace-Linked Logging
# ─────────────────────────────────────────────

def log_with_trace(
    log_file: Path,
    trace_id: str,
    phase: str,
    status: str,
    refusal_reason: Optional[str] = None,
    extra: Optional[Dict[str, Any]] = None,
) -> None:
    """
    Log entry with trace_id attached.
    
    Every log entry MUST include trace_id.
    """
    entry = {
        "trace_id": trace_id,
        "timestamp": datetime.utcnow().isoformat(),
        "phase": phase,
        "status": status,
    }
    
    if refusal_reason:
        entry["refusal_reason"] = refusal_reason
    
    if extra:
        entry.update(extra)
    
    try:
        log_file.parent.mkdir(parents=True, exist_ok=True)
        with open(log_file, 'a', encoding='utf-8') as f:
            f.write(json.dumps(entry, ensure_ascii=False) + '\n')
    except Exception:
        pass  # Don't fail on logging errors
