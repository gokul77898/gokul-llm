"""
Quantization Pipeline (INT4/INT8/FP8)

Implements model quantization for reduced memory usage and faster inference.
Supports multiple quantization schemes with fallbacks for Mac/CPU environments.
"""

import torch
import torch.nn as nn
import logging
from typing import Dict, Any, Optional, Union, List
from dataclasses import dataclass
from pathlib import Path
import json
import time

logger = logging.getLogger(__name__)


@dataclass
class QuantizationConfig:
    """Configuration for model quantization"""
    method: str = "dynamic"  # dynamic, static, qat (quantization-aware training)
    precision: str = "int8"  # int4, int8, fp8, fp16
    calibration_dataset_size: int = 100
    enable_cuda_kernels: bool = True
    preserve_accuracy: bool = True
    target_accuracy_drop: float = 0.02  # Maximum acceptable accuracy drop
    cache_dir: str = "quantization_cache"


class ModelQuantizer:
    """
    Model Quantization Engine
    
    Provides quantization capabilities with automatic fallback for unsupported environments.
    """
    
    def __init__(self, config: QuantizationConfig):
        self.config = config
        self.is_available = self._check_quantization_availability()
        self.quantized_models = {}
        
        # Create cache directory
        Path(config.cache_dir).mkdir(parents=True, exist_ok=True)
        
        if self.is_available:
            logger.info("‚úÖ Quantization available")
        else:
            logger.info("‚ö†Ô∏è  Advanced quantization not available - using fallback")
        
        logger.info(f"   Target precision: {config.precision}")
        logger.info(f"   Method: {config.method}")
    
    def _check_quantization_availability(self) -> bool:
        """Check if advanced quantization is available"""
        try:
            # Check for PyTorch quantization support
            import torch.quantization as quant
            
            # Check for optional advanced quantization libraries
            advanced_available = False
            try:
                import bitsandbytes as bnb
                advanced_available = True
                logger.info("BitsAndBytes available for advanced quantization")
            except ImportError:
                pass
            
            try:
                import auto_gptq
                advanced_available = True
                logger.info("AutoGPTQ available for GPTQ quantization")
            except ImportError:
                pass
            
            return True
            
        except ImportError:
            logger.warning("PyTorch quantization not available")
            return False
    
    def quantize_model(
        self,
        model: nn.Module,
        model_name: str = "quantized_model",
        calibration_data: Optional[torch.Tensor] = None
    ) -> Union[nn.Module, None]:
        """
        Quantize a PyTorch model
        
        Args:
            model: Model to quantize
            model_name: Name for the quantized model
            calibration_data: Calibration data for static quantization
            
        Returns:
            Quantized model or None if quantization fails
        """
        if not self.is_available:
            logger.warning("Quantization not available - returning original model")
            return model
        
        try:
            logger.info(f"üîß Quantizing model '{model_name}'")
            logger.info(f"   Method: {self.config.method}")
            logger.info(f"   Precision: {self.config.precision}")
            
            start_time = time.time()
            
            # Choose quantization method
            if self.config.method == "dynamic":
                quantized_model = self._dynamic_quantization(model)
            elif self.config.method == "static":
                quantized_model = self._static_quantization(model, calibration_data)
            elif self.config.method == "qat":
                quantized_model = self._quantization_aware_training(model)
            else:
                logger.warning(f"Unknown quantization method: {self.config.method}")
                return model
            
            quantization_time = time.time() - start_time
            
            # Validate quantized model
            if self._validate_quantized_model(model, quantized_model):
                self.quantized_models[model_name] = quantized_model
                
                # Calculate model size reduction
                original_size = self._get_model_size(model)
                quantized_size = self._get_model_size(quantized_model)
                size_reduction = (original_size - quantized_size) / original_size * 100
                
                logger.info(f"‚úÖ Quantization complete")
                logger.info(f"   Time: {quantization_time:.2f}s")
                logger.info(f"   Size reduction: {size_reduction:.1f}%")
                logger.info(f"   Original: {original_size:.1f}MB")
                logger.info(f"   Quantized: {quantized_size:.1f}MB")
                
                return quantized_model
            else:
                logger.error("‚ùå Quantized model validation failed")
                return model
                
        except Exception as e:
            logger.error(f"‚ùå Quantization failed: {e}")
            return model
    
    def _dynamic_quantization(self, model: nn.Module) -> nn.Module:
        """Apply dynamic quantization"""
        try:
            import torch.quantization as quant
            
            # Prepare model for quantization
            model.eval()
            
            if self.config.precision == "int8":
                # Standard INT8 dynamic quantization
                quantized_model = torch.quantization.quantize_dynamic(
                    model,
                    {nn.Linear, nn.Conv2d},
                    dtype=torch.qint8
                )
            elif self.config.precision == "fp16":
                # FP16 quantization (half precision)
                quantized_model = model.half()
            else:
                logger.warning(f"Precision {self.config.precision} not supported for dynamic quantization")
                return model
            
            return quantized_model
            
        except Exception as e:
            logger.error(f"Dynamic quantization failed: {e}")
            return model
    
    def _static_quantization(self, model: nn.Module, calibration_data: Optional[torch.Tensor]) -> nn.Module:
        """Apply static quantization with calibration"""
        try:
            import torch.quantization as quant
            
            if calibration_data is None:
                logger.warning("No calibration data provided for static quantization")
                return self._dynamic_quantization(model)
            
            # Prepare model for static quantization
            model.eval()
            model.qconfig = torch.quantization.get_default_qconfig('fbgemm')
            
            # Prepare model
            prepared_model = torch.quantization.prepare(model)
            
            # Calibration
            logger.info("Running calibration...")
            with torch.no_grad():
                for i in range(min(self.config.calibration_dataset_size, calibration_data.shape[0])):
                    prepared_model(calibration_data[i:i+1])
            
            # Convert to quantized model
            quantized_model = torch.quantization.convert(prepared_model)
            
            return quantized_model
            
        except Exception as e:
            logger.error(f"Static quantization failed: {e}")
            return self._dynamic_quantization(model)
    
    def _quantization_aware_training(self, model: nn.Module) -> nn.Module:
        """Apply quantization-aware training setup"""
        try:
            import torch.quantization as quant
            
            # This is a simplified QAT setup
            # In practice, you would need to retrain the model
            model.train()
            model.qconfig = torch.quantization.get_default_qat_qconfig('fbgemm')
            
            # Prepare for QAT
            prepared_model = torch.quantization.prepare_qat(model)
            
            logger.info("‚ö†Ô∏è  QAT model prepared - requires retraining")
            logger.info("   Use this model for training, then convert with torch.quantization.convert()")
            
            return prepared_model
            
        except Exception as e:
            logger.error(f"QAT preparation failed: {e}")
            return model
    
    def _validate_quantized_model(self, original_model: nn.Module, quantized_model: nn.Module) -> bool:
        """Validate that quantized model works correctly"""
        try:
            # Create test input
            test_input = torch.randn(1, 128, dtype=torch.long)  # Adjust based on your model
            
            # Test original model
            original_model.eval()
            with torch.no_grad():
                original_output = original_model(test_input)
            
            # Test quantized model
            quantized_model.eval()
            with torch.no_grad():
                quantized_output = quantized_model(test_input)
            
            # Check output shapes match
            if original_output.shape != quantized_output.shape:
                logger.error("Output shapes don't match")
                return False
            
            # Check for reasonable output values (not NaN/Inf)
            if torch.isnan(quantized_output).any() or torch.isinf(quantized_output).any():
                logger.error("Quantized model produces NaN/Inf values")
                return False
            
            logger.info("‚úÖ Quantized model validation passed")
            return True
            
        except Exception as e:
            logger.error(f"Model validation failed: {e}")
            return False
    
    def _get_model_size(self, model: nn.Module) -> float:
        """Get model size in MB"""
        param_size = 0
        buffer_size = 0
        
        for param in model.parameters():
            param_size += param.nelement() * param.element_size()
        
        for buffer in model.buffers():
            buffer_size += buffer.nelement() * buffer.element_size()
        
        size_mb = (param_size + buffer_size) / 1024 / 1024
        return size_mb
    
    def benchmark_quantized_model(
        self,
        original_model: nn.Module,
        quantized_model: nn.Module,
        test_input: torch.Tensor,
        num_runs: int = 100
    ) -> Dict[str, Any]:
        """Benchmark quantized vs original model"""
        try:
            # Benchmark original model
            original_times = []
            original_model.eval()
            
            for _ in range(10):  # Warmup
                with torch.no_grad():
                    _ = original_model(test_input)
            
            for _ in range(num_runs):
                start_time = time.time()
                with torch.no_grad():
                    _ = original_model(test_input)
                original_times.append(time.time() - start_time)
            
            # Benchmark quantized model
            quantized_times = []
            quantized_model.eval()
            
            for _ in range(10):  # Warmup
                with torch.no_grad():
                    _ = quantized_model(test_input)
            
            for _ in range(num_runs):
                start_time = time.time()
                with torch.no_grad():
                    _ = quantized_model(test_input)
                quantized_times.append(time.time() - start_time)
            
            # Calculate statistics
            original_avg = sum(original_times) / len(original_times) * 1000  # ms
            quantized_avg = sum(quantized_times) / len(quantized_times) * 1000  # ms
            speedup = original_avg / quantized_avg
            
            # Model sizes
            original_size = self._get_model_size(original_model)
            quantized_size = self._get_model_size(quantized_model)
            size_reduction = (original_size - quantized_size) / original_size * 100
            
            return {
                "original_latency_ms": original_avg,
                "quantized_latency_ms": quantized_avg,
                "speedup": speedup,
                "original_size_mb": original_size,
                "quantized_size_mb": quantized_size,
                "size_reduction_percent": size_reduction,
                "num_runs": num_runs
            }
            
        except Exception as e:
            logger.error(f"Benchmarking failed: {e}")
            return {"error": str(e)}
    
    def save_quantized_model(self, model_name: str, save_path: str) -> bool:
        """Save quantized model"""
        if model_name not in self.quantized_models:
            logger.error(f"Quantized model '{model_name}' not found")
            return False
        
        try:
            save_path = Path(save_path)
            save_path.mkdir(parents=True, exist_ok=True)
            
            model = self.quantized_models[model_name]
            
            # Save model
            model_path = save_path / f"{model_name}_quantized.pt"
            torch.save(model.state_dict(), model_path)
            
            # Save metadata
            metadata = {
                "model_name": model_name,
                "quantization_method": self.config.method,
                "precision": self.config.precision,
                "model_size_mb": self._get_model_size(model),
                "created_at": time.time()
            }
            
            metadata_path = save_path / f"{model_name}_metadata.json"
            with open(metadata_path, 'w') as f:
                json.dump(metadata, f, indent=2)
            
            logger.info(f"‚úÖ Quantized model saved: {model_path}")
            return True
            
        except Exception as e:
            logger.error(f"Failed to save quantized model: {e}")
            return False
    
    def load_quantized_model(self, model_path: str, model_class: nn.Module) -> Optional[nn.Module]:
        """Load a saved quantized model"""
        try:
            model = model_class()
            model.load_state_dict(torch.load(model_path, map_location='cpu'))
            model.eval()
            
            logger.info(f"‚úÖ Quantized model loaded: {model_path}")
            return model
            
        except Exception as e:
            logger.error(f"Failed to load quantized model: {e}")
            return None


class AdvancedQuantizer:
    """
    Advanced Quantization with External Libraries
    
    Uses libraries like BitsAndBytes, AutoGPTQ for advanced quantization.
    """
    
    def __init__(self):
        self.bnb_available = self._check_bitsandbytes()
        self.gptq_available = self._check_autogptq()
        
        logger.info("üî¨ Advanced Quantizer initialized")
        logger.info(f"   BitsAndBytes: {self.bnb_available}")
        logger.info(f"   AutoGPTQ: {self.gptq_available}")
    
    def _check_bitsandbytes(self) -> bool:
        """Check BitsAndBytes availability"""
        try:
            import bitsandbytes as bnb
            return True
        except ImportError:
            return False
    
    def _check_autogptq(self) -> bool:
        """Check AutoGPTQ availability"""
        try:
            import auto_gptq
            return True
        except ImportError:
            return False
    
    def quantize_with_bnb(
        self,
        model: nn.Module,
        load_in_8bit: bool = True,
        load_in_4bit: bool = False
    ) -> Optional[nn.Module]:
        """Quantize model with BitsAndBytes"""
        if not self.bnb_available:
            logger.warning("BitsAndBytes not available")
            return None
        
        try:
            import bitsandbytes as bnb
            from transformers import BitsAndBytesConfig
            
            # Configure quantization
            if load_in_4bit:
                bnb_config = BitsAndBytesConfig(
                    load_in_4bit=True,
                    bnb_4bit_compute_dtype=torch.float16,
                    bnb_4bit_use_double_quant=True,
                    bnb_4bit_quant_type="nf4"
                )
                logger.info("üîß Applying 4-bit quantization with BitsAndBytes")
            else:
                bnb_config = BitsAndBytesConfig(
                    load_in_8bit=True,
                    llm_int8_threshold=6.0,
                    llm_int8_has_fp16_weight=False
                )
                logger.info("üîß Applying 8-bit quantization with BitsAndBytes")
            
            # Apply quantization (this would typically be done during model loading)
            logger.info("‚úÖ BitsAndBytes quantization configured")
            return model
            
        except Exception as e:
            logger.error(f"BitsAndBytes quantization failed: {e}")
            return None
    
    def quantize_with_gptq(
        self,
        model: nn.Module,
        calibration_data: torch.Tensor,
        bits: int = 4
    ) -> Optional[nn.Module]:
        """Quantize model with GPTQ"""
        if not self.gptq_available:
            logger.warning("AutoGPTQ not available")
            return None
        
        try:
            # This is a placeholder for GPTQ quantization
            # Actual implementation would require specific model preparation
            logger.info(f"üîß GPTQ quantization to {bits} bits")
            logger.info("‚ö†Ô∏è  GPTQ quantization requires model-specific implementation")
            
            return model
            
        except Exception as e:
            logger.error(f"GPTQ quantization failed: {e}")
            return None


# Factory functions
def create_quantizer(
    method: str = "dynamic",
    precision: str = "int8"
) -> ModelQuantizer:
    """Create model quantizer with specified config"""
    config = QuantizationConfig(
        method=method,
        precision=precision
    )
    return ModelQuantizer(config)


def create_advanced_quantizer() -> AdvancedQuantizer:
    """Create advanced quantizer"""
    return AdvancedQuantizer()


# Integration with existing models
def integrate_quantization():
    """
    Integration function for existing MARK AI models
    
    This can be called to add quantization to existing models.
    """
    try:
        from src.core.mamba_loader import load_mamba_model
        from src.core.model_registry import get_model_instance
        
        logger.info("üîó Integrating quantization with existing models")
        
        def quantize_existing_model(
            model_key: str = "mamba",
            precision: str = "int8",
            method: str = "dynamic"
        ):
            """Quantize existing model"""
            
            # Load model
            if model_key == "mamba":
                model_instance = load_mamba_model()
                if model_instance.available:
                    model = model_instance.model
                else:
                    logger.warning("Mamba model not available")
                    return None
            else:
                model_instance = get_model_instance(model_key)
                if model_instance:
                    model = model_instance.model
                else:
                    logger.warning(f"Model '{model_key}' not available")
                    return None
            
            # Create quantizer
            quantizer = create_quantizer(method=method, precision=precision)
            
            # Quantize model
            quantized_model = quantizer.quantize_model(
                model=model,
                model_name=f"{model_key}_quantized"
            )
            
            if quantized_model:
                logger.info(f"‚úÖ Model '{model_key}' quantized successfully")
                
                # Benchmark if possible
                try:
                    test_input = torch.randint(0, 1000, (1, 128), dtype=torch.long)
                    benchmark_results = quantizer.benchmark_quantized_model(
                        original_model=model,
                        quantized_model=quantized_model,
                        test_input=test_input
                    )
                    
                    logger.info("üìä Quantization Benchmark Results:")
                    for key, value in benchmark_results.items():
                        if isinstance(value, float):
                            logger.info(f"   {key}: {value:.2f}")
                        else:
                            logger.info(f"   {key}: {value}")
                    
                except Exception as e:
                    logger.warning(f"Benchmarking failed: {e}")
                
                return quantized_model
            else:
                logger.error("‚ùå Quantization failed")
                return None
        
        return quantize_existing_model
        
    except ImportError as e:
        logger.warning(f"‚ö†Ô∏è  Could not integrate quantization: {e}")
        return None
